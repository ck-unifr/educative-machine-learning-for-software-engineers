{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-Machine-Learning?\" data-toc-modified-id=\"What-is-Machine-Learning?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>What is Machine Learning?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Supervised-learning\" data-toc-modified-id=\"Supervised-learning-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Supervised learning</a></span></li><li><span><a href=\"#Unsupervised-Learning\" data-toc-modified-id=\"Unsupervised-Learning-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Unsupervised Learning</a></span></li></ul></li><li><span><a href=\"#ML-vs.-AI-vs.-Data-Science\" data-toc-modified-id=\"ML-vs.-AI-vs.-Data-Science-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>ML vs. AI vs. Data Science</a></span></li><li><span><a href=\"#7-Steps-of-the-Machine-Learning-Process\" data-toc-modified-id=\"7-Steps-of-the-Machine-Learning-Process-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>7 Steps of the Machine Learning Process</a></span></li></ul></li><li><span><a href=\"#Data-Manipulation-with-NumPy\" data-toc-modified-id=\"Data-Manipulation-with-NumPy-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Manipulation with NumPy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Arrays\" data-toc-modified-id=\"Arrays-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Arrays</a></span></li><li><span><a href=\"#Basics\" data-toc-modified-id=\"Basics-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#arange\" data-toc-modified-id=\"arange-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>arange</a></span></li><li><span><a href=\"#linspace\" data-toc-modified-id=\"linspace-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>linspace</a></span></li><li><span><a href=\"#reshape\" data-toc-modified-id=\"reshape-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>reshape</a></span></li><li><span><a href=\"#flatten\" data-toc-modified-id=\"flatten-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>flatten</a></span></li><li><span><a href=\"#Transposing\" data-toc-modified-id=\"Transposing-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Transposing</a></span></li><li><span><a href=\"#Zeros-and-ones\" data-toc-modified-id=\"Zeros-and-ones-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Zeros and ones</a></span></li></ul></li><li><span><a href=\"#Math\" data-toc-modified-id=\"Math-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Math</a></span><ul class=\"toc-item\"><li><span><a href=\"#Arithmetic\" data-toc-modified-id=\"Arithmetic-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Arithmetic</a></span></li><li><span><a href=\"#Non-linear-functions\" data-toc-modified-id=\"Non-linear-functions-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Non-linear functions</a></span></li><li><span><a href=\"#Matrix-multiplication\" data-toc-modified-id=\"Matrix-multiplication-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Matrix multiplication</a></span></li></ul></li><li><span><a href=\"#Random\" data-toc-modified-id=\"Random-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Random</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-integers\" data-toc-modified-id=\"Random-integers-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Random integers</a></span></li><li><span><a href=\"#Utility-functions\" data-toc-modified-id=\"Utility-functions-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Utility functions</a></span></li><li><span><a href=\"#Distributions\" data-toc-modified-id=\"Distributions-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Distributions</a></span></li><li><span><a href=\"#Custom-sampling\" data-toc-modified-id=\"Custom-sampling-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Custom sampling</a></span></li></ul></li><li><span><a href=\"#Indexing\" data-toc-modified-id=\"Indexing-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Indexing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Argmin-and-argmax\" data-toc-modified-id=\"Argmin-and-argmax-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Argmin and argmax</a></span></li></ul></li><li><span><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Filtering</a></span></li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Statistics</a></span></li><li><span><a href=\"#Aggregation\" data-toc-modified-id=\"Aggregation-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Aggregation</a></span></li><li><span><a href=\"#Saving-Data\" data-toc-modified-id=\"Saving-Data-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Saving Data</a></span></li></ul></li><li><span><a href=\"#Data-Analysis-with-pandas\" data-toc-modified-id=\"Data-Analysis-with-pandas-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Analysis with pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Series\" data-toc-modified-id=\"Series-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Series</a></span></li><li><span><a href=\"#DataFrame\" data-toc-modified-id=\"DataFrame-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#drop-rows-with-nan\" data-toc-modified-id=\"drop-rows-with-nan-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>drop rows with nan</a></span></li></ul></li><li><span><a href=\"#Combining\" data-toc-modified-id=\"Combining-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Combining</a></span></li><li><span><a href=\"#indexing\" data-toc-modified-id=\"indexing-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>indexing</a></span></li><li><span><a href=\"#File-I/O\" data-toc-modified-id=\"File-I/O-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>File I/O</a></span></li><li><span><a href=\"#Grouping\" data-toc-modified-id=\"Grouping-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Grouping</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Filtering</a></span></li><li><span><a href=\"#Sorting\" data-toc-modified-id=\"Sorting-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Sorting</a></span></li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Metrics</a></span></li><li><span><a href=\"#Plotting\" data-toc-modified-id=\"Plotting-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;</span>Plotting</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing-with-scikit-learn\" data-toc-modified-id=\"Data-Preprocessing-with-scikit-learn-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preprocessing with scikit-learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standardizing-Data\" data-toc-modified-id=\"Standardizing-Data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Standardizing Data</a></span></li><li><span><a href=\"#Data-Range\" data-toc-modified-id=\"Data-Range-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Data Range</a></span></li><li><span><a href=\"#Robust-Scaling\" data-toc-modified-id=\"Robust-Scaling-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Robust Scaling</a></span></li><li><span><a href=\"#Normalizing-Data\" data-toc-modified-id=\"Normalizing-Data-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Normalizing Data</a></span></li><li><span><a href=\"#Data-Imputation\" data-toc-modified-id=\"Data-Imputation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Data Imputation</a></span></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>PCA</a></span></li><li><span><a href=\"#Labeled-Data\" data-toc-modified-id=\"Labeled-Data-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Labeled Data</a></span></li></ul></li><li><span><a href=\"#Data-Modeling-with-scikit-learn\" data-toc-modified-id=\"Data-Modeling-with-scikit-learn-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data Modeling with scikit-learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Linear Regression</a></span></li><li><span><a href=\"#Ridge-Regression\" data-toc-modified-id=\"Ridge-Regression-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Ridge Regression</a></span></li><li><span><a href=\"#LASSO-Regression\" data-toc-modified-id=\"LASSO-Regression-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>LASSO Regression</a></span></li><li><span><a href=\"#Bayesian-Regression\" data-toc-modified-id=\"Bayesian-Regression-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Bayesian Regression</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Decision Trees</a></span></li><li><span><a href=\"#Training-and-Testing\" data-toc-modified-id=\"Training-and-Testing-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Training and Testing</a></span></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Cross-Validation</a></span></li><li><span><a href=\"#Applying-CV-to-Decision-Trees\" data-toc-modified-id=\"Applying-CV-to-Decision-Trees-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Applying CV to Decision Trees</a></span></li><li><span><a href=\"#Evaluating-Models\" data-toc-modified-id=\"Evaluating-Models-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Evaluating Models</a></span></li><li><span><a href=\"#Exhaustive-Tuning\" data-toc-modified-id=\"Exhaustive-Tuning-5.11\"><span class=\"toc-item-num\">5.11&nbsp;&nbsp;</span>Exhaustive Tuning</a></span></li></ul></li><li><span><a href=\"#Clustering-with-scikit-learn\" data-toc-modified-id=\"Clustering-with-scikit-learn-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Clustering with scikit-learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Cosine Similarity</a></span></li><li><span><a href=\"#Nearest-Neighbors\" data-toc-modified-id=\"Nearest-Neighbors-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Nearest Neighbors</a></span></li><li><span><a href=\"#K-Means-Clustering\" data-toc-modified-id=\"K-Means-Clustering-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>K-Means Clustering</a></span></li><li><span><a href=\"#Hierarchical-Clustering\" data-toc-modified-id=\"Hierarchical-Clustering-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Hierarchical Clustering</a></span></li><li><span><a href=\"#Mean-Shift-Clustering\" data-toc-modified-id=\"Mean-Shift-Clustering-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Mean Shift Clustering</a></span></li><li><span><a href=\"#DBSCAN\" data-toc-modified-id=\"DBSCAN-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>DBSCAN</a></span></li><li><span><a href=\"#Evaluating-Clusters\" data-toc-modified-id=\"Evaluating-Clusters-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Evaluating Clusters</a></span></li><li><span><a href=\"#Feature-Clustering\" data-toc-modified-id=\"Feature-Clustering-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Feature Clustering</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "Machine learning is the branch of science that deals with algorithms and systems performing specific tasks using patterns and inference, rather than explicitly programmed instructions. There are a variety of different use cases for machine learning, from image recognition to text generation. Most machine learning tasks generalize to one of the following two learning types:\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Using labeled data to train a model. The labels for the training dataset represent the class/category that each data observation belongs to. After training, the model should be able to predict labels for new data observations (from the same population distribution as the training data).\n",
    "\n",
    "Example: Let’s say you’re training a machine learning model to predict whether a picture contains a lake or not. With supervised learning, you would train a model on a dataset of pictures where the label for each picture is “Yes” if it contains a lake or “No” if it doesn’t. After training, the model will be able to take in a picture and determine whether or not it contains a lake.\n",
    "\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Using unlabeled data to allow a model to learn relationships between data observations and pick up on underlying patterns. Most data in the world is unlabeled, which makes unsupervised learning a very useful method of machine learning.\n",
    "\n",
    "Example: Going back to the same picture dataset from above, but now assume the training dataset is unlabeled. Using unsupervised learning, a model will be able to pick up on the inherent differences between pictures with a lake and pictures without a lake, e.g. differences in pixel color or orientation. This allows the model to cluster the pictures into two separate groups.\n",
    "If it is possible to get large enough labeled training datasets, supervised learning is the way to go. However, it is often difficult to get fully labeled datasets, which is why many tasks require unsupervised learning or semi-supervised learning (a mix of supervised and unsupervised learning). Deciding which type of learning method to use is only the first step towards creating a machine learning model. You also need to choose the proper model architecture for your task and, most importantly, be able to process data into a training pipeline and interpret/analyze model results.\n",
    "\n",
    "## ML vs. AI vs. Data Science\n",
    "\n",
    "People often throw around the terms “machine learning”, “artificial intelligence”, and “data science” interchangeably. In reality, machine learning is a subset of artificial intelligence and overlaps heavily with data science. Artificial intelligence deals with any technique that allows machines to display “intelligence”, similar to humans. Machine learning is one of the main techniques used to create artificial intelligence, but other non-ML techniques (e.g. alpha-beta pruning, rule-based systems) are also widely used in AI.\n",
    "\n",
    "On the other hand, data science deals with gathering insights from datasets. Traditionally, data scientists have used statistical methods for gathering these insights. However, as machine learning continues to grow, it has also penetrated into the field of data science.\n",
    "\n",
    "In industry, any data scientist or AI researcher needs to have a good understanding of machine learning. Machine learning in industry has allowed us to create wonderful autonomous systems. These systems have matched, or sometimes even exceeded, the best human performance in their respective fields. A good example is AlphaGo, a machine-learning based system that has beaten the best human Go players in the world.\n",
    "\n",
    "## 7 Steps of the Machine Learning Process\n",
    "\n",
    "Data Collection: The process of extracting raw datasets for the machine learning task. This data can come from a variety of places, ranging from open-source online resources to paid crowdsourcing. The first step of the machine learning process is arguably the most important. If the data you collect is poor quality or irrelevant, then the model you train will be poor quality as well.\n",
    "\n",
    "- Data Processing and Preparation: Once you’ve gathered the relevant data, you need to process it and make sure that it is in a usable format for training a machine learning model. This includes handling missing data, dealing with outliers, etc.\n",
    "\n",
    "- Feature Engineering: Once you’ve collected and processed your dataset, you will likely need to transform some of the features (and sometimes even drop some features) in order to optimize how well a model can be trained on the data.\n",
    "\n",
    "- Model Selection: Based on the dataset, you will choose which model architecture to use. This is one of the main tasks of industry engineers. Rather than attempting to come up with a completely novel model architecture, most tasks can be thoroughly performed with an existing architecture (or combination of model architectures).\n",
    "\n",
    "- Model Training and Data Pipeline: After selecting the model architecture, you will create a data pipeline for training the model. This means creating a continuous stream of batched data observations to efficiently train the model. Since training can take a long time, you want your data pipeline to be as efficient as possible.\n",
    "\n",
    "- Model Validation: After training the model for a sufficient amount of time, you will need to validate the model’s performance on a held-out portion of the overall dataset. This data needs to come from the same underlying distribution as the training dataset, but needs to be different data that the model has not seen before.\n",
    "\n",
    "- Model Persistence: Finally, after training and validating the model’s performance, you need to be able to properly save the model weights and possibly push the model to production. This means setting up a process with which new users can easily use your pre-trained model to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation with NumPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-1.,  2.,  5.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # import the NumPy library\n",
    "\n",
    "# Initializing a NumPy array\n",
    "arr = np.array([-1, 2, 5], dtype=np.float32)\n",
    "\n",
    "# Print the representation of the array\n",
    "print(repr(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[0 1 2]\n",
      "float32\n",
      "[0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0, 1, 2])\n",
    "print(arr.dtype)\n",
    "print(arr)\n",
    "\n",
    "arr = arr.astype(np.float32)\n",
    "print(arr.dtype)\n",
    "print(arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([nan,  1.,  2.])\n",
      "[nan  1.  2.]\n",
      "array(['nan', 'abc'], dtype='<U32')\n",
      "['nan' 'abc']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2873c0c2e2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Will result in a ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "arr = np.array([np.nan, 1, 2])\n",
    "print(repr(arr))\n",
    "print(arr)\n",
    "\n",
    "arr = np.array([np.nan, 'abc'])\n",
    "print(repr(arr))\n",
    "print(arr)\n",
    "\n",
    "# Will result in a ValueError\n",
    "np.array([np.nan, 1, 2], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infinity\n",
    "\n",
    "To represent infinity in NumPy, we use the np.inf special value. We can also represent negative infinity with -np.inf.\n",
    "\n",
    "The code below shows an example usage of np.inf. Note that np.inf cannot take on an integer type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "array([inf,  5.])\n",
      "[inf  5.]\n",
      "array([-inf,   1.])\n",
      "[-inf   1.]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d28686a502be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Will result in an OverflowError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "print(np.inf > 1000000)\n",
    "\n",
    "arr = np.array([np.inf, 5])\n",
    "print(repr(arr))\n",
    "print(arr)\n",
    "\n",
    "arr = np.array([-np.inf, 1])\n",
    "print(repr(arr))\n",
    "print(arr)\n",
    "\n",
    "# Will result in an OverflowError\n",
    "np.array([np.inf, 3], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([np.nan, 2, 3, 4, 5])\n",
    "\n",
    "arr2 = arr.copy()\n",
    "arr2[0] = 10\n",
    "\n",
    "print(arr[0])\n",
    "print(arr2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  5.4 3. ]\n",
      "[10.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "loat_arr = np.array([1, 5.4, 3])\n",
    "float_arr2 = arr2.astype(np.float32)\n",
    "\n",
    "print(loat_arr)\n",
    "print(float_arr2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1, 2, 3], [4, 5, 6]],\n",
    "                  dtype=np.float32)\n",
    "\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 1, 2, 3, 4])\n",
      "array([0., 1., 2., 3., 4., 5.])\n",
      "array([-1,  0,  1,  2,  3])\n",
      "array([-1.5,  0.5,  2.5])\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(5)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.arange(5.1)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.arange(-1, 4)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.arange(-1.5, 4, 2)\n",
    "print(repr(arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the number of elements in the returned array, rather than the step size, we can use the np.linspace function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 5.,  7.,  9., 11.])\n",
      "array([5. , 6.5, 8. , 9.5])\n",
      "array([ 5,  7,  9, 11], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.linspace(5, 11, num=4)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.linspace(5, 11, num=4, endpoint=False)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.linspace(5, 11, num=4, dtype=np.int32)\n",
    "print(repr(arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 2, 3],\n",
      "       [4, 5, 6, 7]])\n",
      "New shape: (2, 4)\n",
      "array([[[0, 1],\n",
      "        [2, 3]],\n",
      "\n",
      "       [[4, 5],\n",
      "        [6, 7]]])\n",
      "New shape: (2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(8)\n",
    "\n",
    "reshaped_arr = np.reshape(arr, (2, 4))\n",
    "print(repr(reshaped_arr))\n",
    "print('New shape: {}'.format(reshaped_arr.shape))\n",
    "\n",
    "reshaped_arr = np.reshape(arr, (-1, 2, 2))\n",
    "print(repr(reshaped_arr))\n",
    "print('New shape: {}'.format(reshaped_arr.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 2, 3],\n",
      "       [4, 5, 6, 7]])\n",
      "arr shape: (2, 4)\n",
      "array([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "flattened shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(8)\n",
    "arr = np.reshape(arr, (2, 4))\n",
    "flattened = arr.flatten()\n",
    "print(repr(arr))\n",
    "print('arr shape: {}'.format(arr.shape))\n",
    "print(repr(flattened))\n",
    "print('flattened shape: {}'.format(flattened.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]])\n",
      "arr shape: (4, 2)\n",
      "array([[0, 2, 4, 6],\n",
      "       [1, 3, 5, 7]])\n",
      "transposed shape: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(8)\n",
    "arr = np.reshape(arr, (4, 2))\n",
    "transposed = np.transpose(arr)\n",
    "print(repr(arr))\n",
    "print('arr shape: {}'.format(arr.shape))\n",
    "print(repr(transposed))\n",
    "print('transposed shape: {}'.format(transposed.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in a required first argument, which will be the array we want to transpose. It also has a single keyword argument called axes, which represents the new permutation of the dimensions.\n",
    "\n",
    "The permutation is a tuple/list of integers, with the same length as the number of dimensions in the array. It tells us where to switch up the dimensions. For example, if the permutation had 3 at index 1, it means the old third dimension of the data becomes the new second dimension (since index 1 represents the second dimension).\n",
    "\n",
    "The code below shows an example usage of the np.transpose function with the axes keyword argument. The shape property gives us the shape of an array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]]\n",
      "\n",
      " [[ 8  9]\n",
      "  [10 11]\n",
      "  [12 13]\n",
      "  [14 15]]\n",
      "\n",
      " [[16 17]\n",
      "  [18 19]\n",
      "  [20 21]\n",
      "  [22 23]]]\n",
      "[[[ 0  8 16]\n",
      "  [ 1  9 17]]\n",
      "\n",
      " [[ 2 10 18]\n",
      "  [ 3 11 19]]\n",
      "\n",
      " [[ 4 12 20]\n",
      "  [ 5 13 21]]\n",
      "\n",
      " [[ 6 14 22]\n",
      "  [ 7 15 23]]]\n",
      "arr shape: (3, 4, 2)\n",
      "transposed shape: (4, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(24)\n",
    "arr = np.reshape(arr, (3, 4, 2))\n",
    "print(arr)\n",
    "transposed = np.transpose(arr, axes=(1, 2, 0))\n",
    "print(transposed)\n",
    "\n",
    "print('arr shape: {}'.format(arr.shape))\n",
    "print('transposed shape: {}'.format(transposed.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros and ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 0., 0., 0.])\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.]])\n",
      "array([[1, 1, 1],\n",
      "       [1, 1, 1]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.zeros(4)\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.ones((2, 3))\n",
    "print(repr(arr))\n",
    "\n",
    "arr = np.ones((2, 3), dtype=np.int32)\n",
    "print(repr(arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create an array of 0's or 1's with the same shape as another array, we can use np.zeros_like and np.ones_like.\n",
    "\n",
    "The code below shows example usages of np.zeros_like and np.ones_like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0],\n",
      "       [0, 0]])\n",
      "array([[1., 1.],\n",
      "       [1., 1.]])\n",
      "array([[1, 1],\n",
      "       [1, 1]], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "print(repr(np.zeros_like(arr)))\n",
    "\n",
    "arr = np.array([[0., 1.], [1.2, 4.]])\n",
    "print(repr(np.ones_like(arr)))\n",
    "print(repr(np.ones_like(arr, dtype=np.int32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]]\n",
      "\n",
      " [[ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(12)\n",
    "reshaped = np.reshape(arr, (2, 3, 2))\n",
    "\n",
    "print(arr)\n",
    "print(reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "[[[ 0  6]\n",
      "  [ 1  7]]\n",
      "\n",
      " [[ 2  8]\n",
      "  [ 3  9]]\n",
      "\n",
      " [[ 4 10]\n",
      "  [ 5 11]]]\n"
     ]
    }
   ],
   "source": [
    "flattened = reshaped.flatten()\n",
    "transposed = np.transpose(reshaped, axes=(1, 2, 0))\n",
    "\n",
    "\n",
    "print(flattened)\n",
    "print(transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[[[1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]]]\n"
     ]
    }
   ],
   "source": [
    "zeros_arr = np.zeros(5)\n",
    "ones_arr = np.ones_like(transposed)\n",
    "\n",
    "print(zeros_arr)\n",
    "print(ones_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.5  -3.45 -3.4  -3.35 -3.3  -3.25 -3.2  -3.15 -3.1  -3.05 -3.   -2.95\n",
      " -2.9  -2.85 -2.8  -2.75 -2.7  -2.65 -2.6  -2.55 -2.5  -2.45 -2.4  -2.35\n",
      " -2.3  -2.25 -2.2  -2.15 -2.1  -2.05 -2.   -1.95 -1.9  -1.85 -1.8  -1.75\n",
      " -1.7  -1.65 -1.6  -1.55 -1.5  -1.45 -1.4  -1.35 -1.3  -1.25 -1.2  -1.15\n",
      " -1.1  -1.05 -1.   -0.95 -0.9  -0.85 -0.8  -0.75 -0.7  -0.65 -0.6  -0.55\n",
      " -0.5  -0.45 -0.4  -0.35 -0.3  -0.25 -0.2  -0.15 -0.1  -0.05  0.    0.05\n",
      "  0.1   0.15  0.2   0.25  0.3   0.35  0.4   0.45  0.5   0.55  0.6   0.65\n",
      "  0.7   0.75  0.8   0.85  0.9   0.95  1.    1.05  1.1   1.15  1.2   1.25\n",
      "  1.3   1.35  1.4   1.45  1.5 ]\n"
     ]
    }
   ],
   "source": [
    "points = np.linspace(-3.5, 1.5, num=101)\n",
    "\n",
    "print(points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2, 3],\n",
      "       [4, 5]])\n",
      "array([[-0.2,  0.8],\n",
      "       [ 1.8,  2.8]])\n",
      "array([[2, 4],\n",
      "       [6, 8]])\n",
      "array([[0.5, 1. ],\n",
      "       [1.5, 2. ]])\n",
      "array([[0, 1],\n",
      "       [1, 2]])\n",
      "array([[ 1,  4],\n",
      "       [ 9, 16]])\n",
      "array([[1.        , 1.41421356],\n",
      "       [1.73205081, 2.        ]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "# Add 1 to element values\n",
    "print(repr(arr + 1))\n",
    "# Subtract element values by 1.2\n",
    "print(repr(arr - 1.2))\n",
    "# Double element values\n",
    "print(repr(arr * 2))\n",
    "# Halve element values\n",
    "print(repr(arr / 2))\n",
    "# Integer division (half)\n",
    "print(repr(arr // 2))\n",
    "# Square element values\n",
    "print(repr(arr**2))\n",
    "# Square root element values\n",
    "print(repr(arr**0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celsius: array([  0., -20., -10., -40.])\n"
     ]
    }
   ],
   "source": [
    "def f2c(temps):\n",
    "    return (5/9)*(temps-32)\n",
    "\n",
    "fahrenheits = np.array([32, -4, 14, -40])\n",
    "celsius = f2c(fahrenheits)\n",
    "print('Celsius: {}'.format(repr(celsius)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 2.71828183,  7.3890561 ],\n",
      "       [20.08553692, 54.59815003]])\n",
      "array([[ 2.,  4.],\n",
      "       [ 8., 16.]])\n",
      "array([[0.        , 2.30258509],\n",
      "       [1.        , 1.14472989]])\n",
      "array([[0.        , 1.        ],\n",
      "       [0.43429448, 0.49714987]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "# Raised to power of e\n",
    "print(repr(np.exp(arr)))\n",
    "# Raised to power of 2\n",
    "print(repr(np.exp2(arr)))\n",
    "\n",
    "arr2 = np.array([[1, 10], [np.e, np.pi]])\n",
    "# Natural logarithm\n",
    "print(repr(np.log(arr2)))\n",
    "# Base 10 logarithm\n",
    "print(repr(np.log10(arr2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 3,  9],\n",
      "       [27, 81]])\n",
      "array([[ 10.2,  16. ],\n",
      "       [ 27. , 625. ]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "# Raise 3 to power of each number in arr\n",
    "print(repr(np.power(3, arr)))\n",
    "arr2 = np.array([[10.2, 4], [3, 5]])\n",
    "# Raise arr2 to power of each number in arr\n",
    "print(repr(np.power(arr2, arr)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/stable/reference/routines.math.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "array([[  5,   4,  -7],\n",
      "       [  9,   8, -13],\n",
      "       [ 13,  12, -19]])\n",
      "[[  5   4  -7]\n",
      " [  9   8 -13]\n",
      " [ 13  12 -19]]\n",
      "array([[  4,   4],\n",
      "       [-11, -10]])\n",
      "[[  4   4]\n",
      " [-11 -10]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-72557c3d6f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# This will result in ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "arr2 = np.array([-3, 0, 10])\n",
    "print(np.matmul(arr1, arr2))\n",
    "\n",
    "arr3 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "arr4 = np.array([[-1, 0, 1], [3, 2, -4]])\n",
    "print(repr(np.matmul(arr3, arr4)))\n",
    "print(np.matmul(arr3, arr4))\n",
    "print(repr(np.matmul(arr4, arr3)))\n",
    "print(np.matmul(arr4, arr3))\n",
    "# This will result in ValueError\n",
    "print(repr(np.matmul(arr3, arr3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5  0.8 -0.1]\n",
      " [ 0.  -1.2  1.3]]\n",
      "[[1.2 3.1]\n",
      " [1.2 0.3]\n",
      " [1.5 2.2]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-0.5, 0.8, -0.1], [0.0, -1.2, 1.3]])\n",
    "arr2 = np.array([[1.2, 3.1], [1.2, 0.3], [1.5, 2.2]])\n",
    "\n",
    "print(arr)\n",
    "print(arr2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.57079633  2.51327412 -0.31415927]\n",
      " [ 0.         -3.76991118  4.08407045]]\n",
      "[[-2.07079633  3.31327412 -0.41415927]\n",
      " [ 0.         -4.96991118  5.38407045]]\n",
      "[[ 4.28819743 10.97778541  0.1715279 ]\n",
      " [ 0.         24.70001718 28.98821461]]\n"
     ]
    }
   ],
   "source": [
    "multiplied = arr * np.pi\n",
    "added = arr + multiplied\n",
    "squared = added**2\n",
    "\n",
    "print(multiplied)\n",
    "print(added)\n",
    "print(squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.28350596e+01 5.85587272e+04 1.18711726e+00]\n",
      " [1.00000000e+00 5.33434578e+10 3.88527393e+12]]\n",
      "[[ 0.18232156  1.13140211]\n",
      " [ 0.18232156 -1.2039728 ]\n",
      " [ 0.40546511  0.78845736]]\n"
     ]
    }
   ],
   "source": [
    "exponential = np.exp(squared)\n",
    "logged = np.log(arr2)\n",
    "\n",
    "print(exponential)\n",
    "print(logged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.44108036e+01  6.03529115e+10  4.39580713e+12]\n",
      " [ 1.20754286e+01 -6.42240618e+10 -4.67776415e+12]\n",
      " [ 3.03205327e+01  4.20590657e+10  3.06337283e+12]]\n",
      "[[ 1.06902790e+04 -7.04197733e+04]\n",
      " [ 1.58506868e+12  2.99914875e+12]]\n"
     ]
    }
   ],
   "source": [
    "matmul1 = np.matmul(logged, exponential)\n",
    "matmul2 = np.matmul(exponential, logged)\n",
    "\n",
    "print(matmul1)\n",
    "print(matmul2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "5\n",
      "array([[ 7,  1],\n",
      "       [ 3, -2]])\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint(5))\n",
    "print(np.random.randint(5))\n",
    "print(np.random.randint(5, high=6))\n",
    "\n",
    "random_arr = np.random.randint(-3, high=14,\n",
    "                               size=(2, 2))\n",
    "print(repr(random_arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The np.random.randint function takes in a single required argument, which actually depends on the high keyword argument. If high=None (which is the default value), then the required argument represents the upper (exclusive) end of the range, with the lower end being 0. Specifically, if the required argument is n, then the random integer is chosen uniformly from the range [0, n).\n",
    "\n",
    "If high is not None, then the required argument will represent the lower (inclusive) end of the range, while high represents the upper (exclusive) end.\n",
    "\n",
    "The size keyword argument specifies the size of the output array, where each integer in the array is randomly drawn from the specified range. As a default, np.random.randint returns a single integer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "array([[15, 75],\n",
      "       [12, 78]])\n",
      "8\n",
      "array([[18, 75],\n",
      "       [25, 46]])\n",
      "5\n",
      "array([[15, 75],\n",
      "       [12, 78]])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(np.random.randint(10))\n",
    "random_arr = np.random.randint(3, high=100,\n",
    "                               size=(2, 2))\n",
    "print(repr(random_arr))\n",
    "\n",
    "# New seed\n",
    "np.random.seed(2)\n",
    "print(np.random.randint(10))\n",
    "random_arr = np.random.randint(3, high=100,\n",
    "                               size=(2, 2))\n",
    "print(repr(random_arr))\n",
    "\n",
    "# Original seed\n",
    "np.random.seed(1)\n",
    "print(np.random.randint(10))\n",
    "random_arr = np.random.randint(3, high=100,\n",
    "                               size=(2, 2))\n",
    "print(repr(random_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([3, 4, 2, 5, 1])\n",
      "array([5, 3, 4, 2, 1])\n",
      "array([[4, 5, 6],\n",
      "       [7, 8, 9],\n",
      "       [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "vec = np.array([1, 2, 3, 4, 5])\n",
    "np.random.shuffle(vec)\n",
    "print(repr(vec))\n",
    "np.random.shuffle(vec)\n",
    "print(repr(vec))\n",
    "\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "np.random.shuffle(matrix)\n",
    "print(repr(matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3132735169322751\n",
      "0.4408281904196243\n",
      "array([0.44345289, 0.22957721, 0.53441391])\n",
      "array([[5.09984683, 0.85200471],\n",
      "       [0.60549667, 5.33388844]])\n"
     ]
    }
   ],
   "source": [
    "print(np.random.uniform())\n",
    "print(np.random.uniform(low=-1.5, high=2.2))\n",
    "print(repr(np.random.uniform(size=3)))\n",
    "print(repr(np.random.uniform(low=-3.4, high=5.9,\n",
    "                             size=(2, 2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function np.random.uniform actually has no required arguments. The keyword arguments, low and high, represent the inclusive lower end and exclusive upper end from which to draw random samples. Since they have default values of 0.0 and 1.0, respectively, the default outputs of np.random.uniform come from the range [0.0, 1.0).\n",
    "\n",
    "The size keyword argument is the same as the one for np.random.randint, i.e. it represents the output size of the array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7252740646272712\n",
      "4.772112039383628\n",
      "array([[ 2.07318791, -2.17754724],\n",
      "       [-0.89337346, -0.89545991]])\n"
     ]
    }
   ],
   "source": [
    "print(np.random.normal())\n",
    "print(np.random.normal(loc=1.5, scale=3.5))\n",
    "print(repr(np.random.normal(loc=-2.4, scale=4.0,\n",
    "                            size=(2, 2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like np.random.uniform, np.random.normal has no required arguments. The loc and scale keyword arguments represent the mean and standard deviation, respectively, of the normal distribution we sample from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "array(['blue', 'red'], dtype='<U5')\n",
      "array([['red', 'red'],\n",
      "       ['blue', 'red']], dtype='<U5')\n"
     ]
    }
   ],
   "source": [
    "colors = ['red', 'blue', 'green']\n",
    "print(np.random.choice(colors))\n",
    "print(repr(np.random.choice(colors, size=2)))\n",
    "print(repr(np.random.choice(colors, size=(2, 2),\n",
    "                            p=[0.8, 0.19, 0.01])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[3 4 6 7 5]\n",
      " [7 3 8 6 4]\n",
      " [5 3 7 4 5]]\n"
     ]
    }
   ],
   "source": [
    "random1 = np.random.randint(5)\n",
    "random_arr = np.random.randint(3, high=10, size=(3, 5))\n",
    "\n",
    "\n",
    "print(random1)\n",
    "print(random_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65711731 -2.08709597 -0.7084259   1.13438201 -1.32554341]\n"
     ]
    }
   ],
   "source": [
    "random_uniform = np.random.uniform(low=-2.5, high=1.5, size=5)\n",
    "\n",
    "print(random_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42081263  0.61136266 -0.40510445 -0.95821975 -0.34936146]\n",
      " [ 1.9556739  -1.91058622  2.82045494  7.80930762  4.59715456]\n",
      " [ 1.32857557 -1.10670137 -0.61505403  7.9235911   2.17782714]\n",
      " [-0.22948476  2.6682042   9.35089298  2.42055633  4.16021088]\n",
      " [ 3.05059612  0.76712554 -1.99881369  0.77730047  1.26887018]\n",
      " [ 4.05318117  4.93644195  5.25885728  2.99955564  5.09799407]\n",
      " [-0.64039279  6.38503854  3.79525437  0.95667508  3.70981351]\n",
      " [ 1.735499    5.96070286  7.31935886  9.64951392 -2.88773717]\n",
      " [-3.05439832  0.23436948  2.56012974  5.06659122  3.10472232]\n",
      " [-5.07770426  0.92828596  4.89791125  2.80533157  4.66703913]]\n"
     ]
    }
   ],
   "source": [
    "random_norm = np.random.normal(loc=2.0, scale=3.5, size=(10, 5))\n",
    "print(random_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "choices = ['a', 'b', 'c', 'd']\n",
    "choice = np.random.choice(choices, p=[0.5, 0.1, 0.2, 0.2])\n",
    "\n",
    "print(choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "array([6, 3])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(arr[0])\n",
    "print(arr[4])\n",
    "\n",
    "arr = np.array([[6, 3], [0, 2]])\n",
    "# Subarray\n",
    "print(repr(arr[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 2, 3, 4, 5])\n",
      "array([2, 3, 4, 5])\n",
      "array([3, 4])\n",
      "array([1, 2, 3, 4])\n",
      "array([4, 5])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(repr(arr[:]))\n",
    "print(repr(arr[1:]))\n",
    "print(repr(arr[2:4]))\n",
    "print(repr(arr[:-1]))\n",
    "print(repr(arr[-2:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmin and argmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-2, -1, -3],\n",
    "                [4, 5, -6],\n",
    "                [-3, 9, 1]])\n",
    "print(np.argmin(arr[0]))\n",
    "print(np.argmax(arr[2]))\n",
    "print(np.argmin(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 0, 1])\n",
      "array([2, 2, 0])\n",
      "array([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-2, -1, -3],\n",
    "                [4, 5, -6],\n",
    "                [-3, 9, 1]])\n",
    "print(repr(np.argmin(arr, axis=0)))\n",
    "print(repr(np.argmin(arr, axis=1)))\n",
    "print(repr(np.argmax(arr, axis=-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[False, False,  True],\n",
      "       [False,  True, False],\n",
      "       [False, False, False]])\n",
      "array([[False,  True,  True],\n",
      "       [ True,  True, False],\n",
      "       [False, False,  True]])\n",
      "array([[ True,  True,  True],\n",
      "       [False,  True,  True],\n",
      "       [ True,  True, False]])\n",
      "array([[False, False, False],\n",
      "       [ True, False, False],\n",
      "       [False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 2, 3],\n",
    "                [1, 3, -6],\n",
    "                [-3, -2, 1]])\n",
    "print(repr(arr == 3))\n",
    "print(repr(arr > 0))\n",
    "print(repr(arr != 1))\n",
    "# Negated from the previous step\n",
    "print(repr(~(arr != 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[False, False,  True],\n",
      "       [False,  True, False],\n",
      "       [ True, False, False]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 2, np.nan],\n",
    "                [1, np.nan, -6],\n",
    "                [np.nan, -2, 1]])\n",
    "print(repr(np.isnan(arr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 2]),)\n",
      "(array([1, 3]),)\n",
      "array([0, 0, 1, 2])\n",
      "array([1, 2, 0, 0])\n",
      "array([ 2,  3,  1, -3])\n"
     ]
    }
   ],
   "source": [
    "print(repr(np.where([True, False, True])))\n",
    "\n",
    "arr = np.array([0, 3, 5, 3, 1])\n",
    "print(repr(np.where(arr == 3)))\n",
    "\n",
    "arr = np.array([[0, 2, 3],\n",
    "                [1, 0, 0],\n",
    "                [-3, 0, 0]])\n",
    "x_ind, y_ind = np.where(arr != 0)\n",
    "print(repr(x_ind)) # x indices of non-zero elements\n",
    "print(repr(y_ind)) # y indices of non-zero elements\n",
    "print(repr(arr[x_ind, y_ind]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The interesting thing about np.where is that it must be applied with exactly 1 or 3 arguments. When we use 3 arguments, the first argument is still the boolean array. However, the next two arguments represent the True replacement values and the False replacement values, respectively. The output of the function now becomes an array with the same shape as the first argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1, -5],\n",
      "       [-1,  4]])\n",
      "array([[-2, -5],\n",
      "       [ 3,  4]])\n",
      "array([[-2, -5],\n",
      "       [-1, -8]])\n"
     ]
    }
   ],
   "source": [
    "np_filter = np.array([[True, False], [False, True]])\n",
    "positives = np.array([[1, 2], [3, 4]])\n",
    "negatives = np.array([[-2, -5], [-1, -8]])\n",
    "print(repr(np.where(np_filter, positives, negatives)))\n",
    "\n",
    "np_filter = positives > 2\n",
    "print(repr(np.where(np_filter, positives, negatives)))\n",
    "\n",
    "np_filter = negatives > 0\n",
    "print(repr(np.where(np_filter, positives, negatives)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1, -1],\n",
      "       [-1,  4]])\n"
     ]
    }
   ],
   "source": [
    "np_filter = np.array([[True, False], [False, True]])\n",
    "positives = np.array([[1, 2], [3, 4]])\n",
    "print(repr(np.where(np_filter, positives, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[False, False, False],\n",
      "       [ True,  True, False],\n",
      "       [ True,  True,  True]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-2, -1, -3],\n",
    "                [4, 5, -6],\n",
    "                [3, 9, 1]])\n",
    "print(repr(arr > 0))\n",
    "print(np.any(arr > 0))\n",
    "print(np.all(arr > 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[False, False, False],\n",
      "       [ True,  True, False],\n",
      "       [ True,  True,  True]])\n",
      "array([ True,  True,  True])\n",
      "array([False,  True,  True])\n",
      "array([False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-2, -1, -3],\n",
    "                [4, 5, -6],\n",
    "                [3, 9, 1]])\n",
    "print(repr(arr > 0))\n",
    "print(repr(np.any(arr > 0, axis=0)))\n",
    "print(repr(np.any(arr > 0, axis=1)))\n",
    "print(repr(np.all(arr > 0, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The np.any function is equivalent to performing a logical OR (||), while the np.all function is equivalent to a logical AND (&&) on the first argument. np.any returns true if even one of the elements in the array meets the condition and np.all returns true only if all the elements meet the condition. When only a single argument is passed in, the function is applied across the entire input array, so the returned value is a single boolean.\n",
    "\n",
    "However, if we use a multi-dimensional input and specify the axis keyword argument, the returned value will be an array. The axis argument has the same meaning as it did for np.argmin and np.argmax from the previous chapter. Using axis=0 means the function finds the index of the minimum row element for each column. When we used axis=1, the function finds the index of the minimum column element for each row.\n",
    "\n",
    "Setting axis to -1 just means we apply the function across the last dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True]\n",
      "array([[ 4,  5, -6],\n",
      "       [ 3,  9,  1]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[-2, -1, -3],\n",
    "                [4, 5, -6],\n",
    "                [3, 9, 1]])\n",
    "has_positive = np.any(arr > 0, axis=1)\n",
    "print(has_positive)\n",
    "print(repr(arr[np.where(has_positive)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-60\n",
      "72\n",
      "array([ -3,  -2, -60])\n",
      "array([72,  3,  4])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 72, 3],\n",
    "                [1, 3, -60],\n",
    "                [-3, -2, 4]])\n",
    "print(arr.min())\n",
    "print(arr.max())\n",
    "\n",
    "print(repr(arr.min(axis=0)))\n",
    "print(repr(arr.max(axis=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ -3,  -2, -60])\n"
     ]
    }
   ],
   "source": [
    "print(repr(arr.min(axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([  0, -60,  -3])\n"
     ]
    }
   ],
   "source": [
    "print(repr(arr.min(axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axis keyword argument is identical to how it was used in np.argmin and np.argmax from the chapter on Indexing. In our example, we use axis=0 to find an array of the minimum values in each column of arr and axis=1 to find an array of the maximum values in each row of arr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "977.3333333333334\n",
      "1.0\n",
      "[0. 3. 3.]\n",
      "[ 3.  1. -2.]\n",
      "[ 3.  1. -2.]\n",
      "array([ 3.,  1., -2.])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 72, 3],\n",
    "                [1, 3, -60],\n",
    "                [-3, -2, 4]])\n",
    "print(np.mean(arr))\n",
    "print(np.var(arr))\n",
    "print(np.median(arr))\n",
    "print(np.median(arr, axis=0))\n",
    "print(np.median(arr, axis=1))\n",
    "print(np.median(arr, axis=-1))\n",
    "print(repr(np.median(arr, axis=-1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "array([ -2,  73, -53])\n",
      "array([ 75, -56,  -1])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 72, 3],\n",
    "                [1, 3, -60],\n",
    "                [-3, -2, 4]])\n",
    "print(np.sum(arr))\n",
    "print(repr(np.sum(arr, axis=0)))\n",
    "print(repr(np.sum(arr, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0, 72, 75, 76, 79, 19, 16, 14, 18])\n",
      "array([[  0,  72,   3],\n",
      "       [  1,  75, -57],\n",
      "       [ -2,  73, -53]])\n",
      "array([[  0,  72,  75],\n",
      "       [  1,   4, -56],\n",
      "       [ -3,  -5,  -1]])\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[0, 72, 3],\n",
    "                [1, 3, -60],\n",
    "                [-3, -2, 4]])\n",
    "print(repr(np.cumsum(arr)))\n",
    "print(repr(np.cumsum(arr, axis=0)))\n",
    "print(repr(np.cumsum(arr, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to regular sums, NumPy can perform cumulative sums using np.cumsum. Like np.sum, np.cumsum also takes in a NumPy array as a required argument and uses the axis argument. If the axis keyword argument is not specified, np.cumsum will return the cumulative sums for the flattened array.\n",
    "\n",
    "The code below shows how to use np.cumsum. For a 2-D NumPy array, setting axis=0 returns an array with cumulative sums across each column, while axis=1 returns the array with cumulative sums across each row. Not setting axis returns a cumulative sum across all the values of the flattened array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  0,  72,   3],\n",
      "       [  1,   3, -60],\n",
      "       [ -3,  -2,   4],\n",
      "       [-15,   6,   1],\n",
      "       [  8,   9,  -4],\n",
      "       [  5, -21,  18]])\n",
      "array([[  0,  72,   3, -15,   6,   1],\n",
      "       [  1,   3, -60,   8,   9,  -4],\n",
      "       [ -3,  -2,   4,   5, -21,  18]])\n",
      "array([[-15,   6,   1,   0,  72,   3],\n",
      "       [  8,   9,  -4,   1,   3, -60],\n",
      "       [  5, -21,  18,  -3,  -2,   4]])\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[0, 72, 3],\n",
    "                 [1, 3, -60],\n",
    "                 [-3, -2, 4]])\n",
    "arr2 = np.array([[-15, 6, 1],\n",
    "                 [8, 9, -4],\n",
    "                 [5, -21, 18]])\n",
    "print(repr(np.concatenate([arr1, arr2])))\n",
    "print(repr(np.concatenate([arr1, arr2], axis=1)))\n",
    "print(repr(np.concatenate([arr2, arr1], axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of aggregation is combining multiple datasets. In NumPy, this equates to combining multiple arrays into one. The function we use to do this is np.concatenate.\n",
    "\n",
    "Like the summation functions, np.concatenate uses the axis keyword argument. However, the default value for axis is 0 (i.e. dimension 0). Furthermore, the required argument for np.concatenate is a list of arrays, which the function combines into a single array.\n",
    "\n",
    "The code below shows how to use np.concatenate, which aggregates arrays by joining them along a specific dimension. For 2-D arrays, not setting the axis argument (defaults to axis=0) concatenates the arrays vertically. When we set axis=1, the arrays are concatenated horizontally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "# Saves to 'arr.npy'\n",
    "#np.save('arr.npy', arr)\n",
    "\n",
    "# Also saves to 'arr.npy'\n",
    "#np.save('arr', arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "#np.save('arr.npy', arr)\n",
    "#load_arr = np.load('arr.npy')\n",
    "#print(repr(load_arr))\n",
    "\n",
    "# Will result in FileNotFoundError\n",
    "#load_arr = np.load('arr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to NumPy, pandas frequently deals with 1-D and 2-D data. However, we use two separate objects to deal with 1-D and 2-D data in pandas. For 1-D data, we use the pandas.Series objects, which we'll refer to simply as a Series.\n",
    "\n",
    "A Series is created through the pd.Series constructor, which takes in no required arguments but does have a variety of keyword arguments.\n",
    "\n",
    "The first keyword argument is data, which specifies the elements of the Series. If data is not set, pd.Series returns an empty Series. Since the data keyword argument is almost always used, we treat it like a regular first argument (i.e. skip the data= prefix).\n",
    "\n",
    "Similar to the np.array constructor, pd.Series also takes in the dtype keyword argument for manual casting.\n",
    "\n",
    "The code below shows how to create pandas Series objects using pd.Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n",
      "\n",
      "0    5\n",
      "dtype: int64\n",
      "\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n",
      "\n",
      "0    1.0\n",
      "1    2.2\n",
      "dtype: float64\n",
      "\n",
      "0    1.0\n",
      "1    2.0\n",
      "dtype: float32\n",
      "\n",
      "0    [1, 2]\n",
      "1    [3, 4]\n",
      "dtype: object\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kai/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "series = pd.Series()\n",
    "# Newline to separate series print statements\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series(5)\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series([1, 2, 3])\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series([1, 2.2]) # upcasting\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "arr = np.array([1, 2])\n",
    "series = pd.Series(arr, dtype=np.float32)\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series([[1, 2], [3, 4]])\n",
    "print('{}\\n'.format(series))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples, you may have noticed the zero-indexed integers to the left of the elements in each Series. These integers are collectively referred to as the index of a Series, and each individual index element is referred to as a label.\n",
    "\n",
    "The default index is integers from 0 to n - 1, where n is the number of elements in the Series. However, we can specify a custom index via the index keyword argument of pd.Series.\n",
    "\n",
    "The code below shows how to use the index keyword argument with pd.Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "dtype: int64\n",
      "\n",
      "a      1\n",
      "8      2\n",
      "0.3    3\n",
      "dtype: int64\n",
      "\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "series = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series([1, 2, 3], index=['a', 8, 0.3])\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "print(series['a'])\n",
    "print(series[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to set the index of a Series is by using a Python dictionary for the data argument. The keys of the dictionary represent the index of the Series, while each individual key is the label for its corresponding value.\n",
    "\n",
    "The code below shows how to use pd.Series with a Python dictionary as the first argument. In our example, we set 'a', 'b', and 'c' as the Series index, with corresponding values 1, 2, and 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "dtype: int64\n",
      "\n",
      "b    2\n",
      "a    1\n",
      "c    3\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "series = pd.Series({'a':1, 'b':2, 'c':3})\n",
    "print('{}\\n'.format(series))\n",
    "\n",
    "series = pd.Series({'b':2, 'a':1, 'c':3})\n",
    "print('{}\\n'.format(series))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main purposes of pandas is to deal with tabular data, i.e. data that comes from tables or spreadsheets. Since tabular data contains rows and columns, it is 2-D. For working with 2-D data, we use the pandas.DataFrame object, which we'll refer to simply as a DataFrame.\n",
    "\n",
    "A DataFrame is created through the pd.DataFrame constructor, which takes in essentially the same arguments as pd.Series. However, while a Series could be constructed from a scalar (representing a single value Series), a DataFrame cannot.\n",
    "\n",
    "Furthermore, pd.DataFrame takes in an additional columns keyword argument, which represents the labels for the columns (similar to how index represents the row labels).\n",
    "\n",
    "The code below shows how to use the pd.DataFrame constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "   0\n",
      "0  5\n",
      "1  6\n",
      "\n",
      "   0  1\n",
      "0  5  6\n",
      "\n",
      "    c1  c2\n",
      "r1   5   6\n",
      "r2   1   3\n",
      "\n",
      "    c1  c2\n",
      "r1   1   3\n",
      "r2   2   4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "# Newline added to separate DataFrames\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "df = pd.DataFrame([5, 6])\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "df = pd.DataFrame([[5,6]])\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "df = pd.DataFrame([[5, 6], [1, 3]],\n",
    "                  index=['r1', 'r2'],\n",
    "                  columns=['c1', 'c2'])\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "df = pd.DataFrame({'c1': [1, 2], 'c2': [3, 4]},\n",
    "                  index=['r1', 'r2'])\n",
    "print('{}\\n'.format(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  1\n",
      "0  5.0  6\n",
      "1  1.2  3\n",
      "\n",
      "0    float64\n",
      "1      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "upcast = pd.DataFrame([[5, 6], [1.2, 3]])\n",
    "print('{}\\n'.format(upcast))\n",
    "# Datatypes of each column\n",
    "print(upcast.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can append additional rows to a given DataFrame through the append function. The required argument for the function is either a Series or DataFrame, representing the row(s) we append.\n",
    "\n",
    "Note that the append function returns the modified DataFrame but doesn't actually change the original. Furthermore, when we append a Series to the DataFrame, we either need to specify the name for the series or use the ignore_index keyword argument. Setting ignore_index=True will change the row labels to integer indexes.\n",
    "\n",
    "The code below shows example usages of the append function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1\n",
      "0   5.0  6\n",
      "1   1.2  3\n",
      "r3  0.0  0\n",
      "\n",
      "     0  1\n",
      "0  5.0  6\n",
      "1  1.2  3\n",
      "2  0.0  0\n",
      "\n",
      "     0  1\n",
      "0  5.0  6\n",
      "1  1.2  3\n",
      "0  0.0  0\n",
      "1  9.0  9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([[5, 6], [1.2, 3]])\n",
    "ser = pd.Series([0, 0], name='r3')\n",
    "\n",
    "df_app = df.append(ser)\n",
    "print('{}\\n'.format(df_app))\n",
    "\n",
    "df_app = df.append(ser, ignore_index=True)\n",
    "print('{}\\n'.format(df_app))\n",
    "\n",
    "df2 = pd.DataFrame([[0,0],[9,9]])\n",
    "df_app = df.append(df2)\n",
    "print('{}\\n'.format(df_app))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop rows or columns from a given DataFrame through the drop function. There is no required argument, but the keyword arguments of the function gives us two ways to drop rows/columns from a DataFrame.\n",
    "\n",
    "The first way is using the labels keyword argument to specify the labels of the rows/columns we want to drop. We use this alongside the axis keyword argument (which has default value of 0) to drop from the rows or columns axis.\n",
    "\n",
    "The second method is to directly use the index or columns keyword arguments to specify the labels of the rows or columns directly, without needing to use axis.\n",
    "\n",
    "The code below shows examples on how to use the drop function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1  c2  c3\n",
      "r2   2   4   6\n",
      "\n",
      "    c2\n",
      "r1   3\n",
      "r2   4\n",
      "\n",
      "    c1  c2  c3\n",
      "r1   1   3   5\n",
      "\n",
      "    c1  c3\n",
      "r1   1   5\n",
      "r2   2   6\n",
      "\n",
      "    c1  c3\n",
      "r1   1   5\n",
      "r2   2   6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'c1': [1, 2], 'c2': [3, 4],\n",
    "                   'c3': [5, 6]},\n",
    "                  index=['r1', 'r2'])\n",
    "# Drop row r1\n",
    "df_drop = df.drop(labels='r1')\n",
    "print('{}\\n'.format(df_drop))\n",
    "\n",
    "# Drop columns c1, c3\n",
    "df_drop = df.drop(labels=['c1', 'c3'], axis=1)\n",
    "print('{}\\n'.format(df_drop))\n",
    "\n",
    "df_drop = df.drop(index='r2')\n",
    "print('{}\\n'.format(df_drop))\n",
    "\n",
    "df_drop = df.drop(columns='c2')\n",
    "print('{}\\n'.format(df_drop))\n",
    "\n",
    "df.drop(index='r2', columns='c2')\n",
    "print('{}\\n'.format(df_drop))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop rows with nan\n",
    "\n",
    "df = df[df['col'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to append, the drop function returns the modified DataFrame but doesn't actually change the original.\n",
    "\n",
    "Note that when using labels and axis, we can't drop both rows and columns from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we discussed the append function for concatenating DataFrame rows. To concatenate multiple DataFrames along either rows or columns, we use the pd.concat function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1  c2  c1  c2\n",
      "r1   1   3   5   7\n",
      "r2   2   4   6   8\n",
      "\n",
      "    c1  c2\n",
      "r1   5   7\n",
      "r2   6   8\n",
      "r1   1   3\n",
      "r2   2   4\n",
      "0    5   7\n",
      "1    6   8\n",
      "\n",
      "     c1   c2   c1   c2\n",
      "r1  1.0  3.0  NaN  NaN\n",
      "r2  2.0  4.0  NaN  NaN\n",
      "0   NaN  NaN  5.0  7.0\n",
      "1   NaN  NaN  6.0  8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'c1':[1,2], 'c2':[3,4]},\n",
    "                   index=['r1','r2'])\n",
    "df2 = pd.DataFrame({'c1':[5,6], 'c2':[7,8]},\n",
    "                   index=['r1','r2'])\n",
    "df3 = pd.DataFrame({'c1':[5,6], 'c2':[7,8]})\n",
    "\n",
    "concat = pd.concat([df1, df2], axis='columns')\n",
    "#concat = pd.concat([df1, df2], axis=1)\n",
    "# Newline to separate print statements\n",
    "print('{}\\n'.format(concat))\n",
    "\n",
    "concat = pd.concat([df2, df1, df3])\n",
    "print('{}\\n'.format(concat))\n",
    "\n",
    "concat = pd.concat([df1, df3], axis=1)\n",
    "print('{}\\n'.format(concat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pd.concat function takes in a list of pandas objects (normally a list of DataFrames) to concatenate. The function also takes in numerous keyword arguments, with axis being one of the more important ones. The axis argument specifies whether we concatenate the rows (axis=0, the default), or concatenate the columns (axis=1).\n",
    "\n",
    "This works very similarly to concatenation in NumPy\n",
    "In the code example, the final call to pd.concat resulted in a DataFrame with many NaN values. This is because the row labels for df1 and df3 did not match, so result was padded with NaN in locations where values did not exist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name pos  year\n",
      "0   john doe  1B  2000\n",
      "1   al smith   C  2004\n",
      "2  sam black   P  2008\n",
      "3   john doe  2B  2003\n",
      "\n",
      "        name pos  year\n",
      "0   john doe  1B  2000\n",
      "1   al smith   C  2004\n",
      "2  sam black   P  2008\n",
      "3   john doe  2B  2003\n",
      "\n",
      "       name pos  year  rbi\n",
      "0  john doe  1B  2000   80\n",
      "1  al smith   C  2004  100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlb_df1 = pd.DataFrame({'name': ['john doe', 'al smith', 'sam black', 'john doe'],\n",
    "                        'pos': ['1B', 'C', 'P', '2B'],\n",
    "                        'year': [2000, 2004, 2008, 2003]})\n",
    "mlb_df2 = pd.DataFrame({'name': ['john doe', 'al smith', 'jack lee'],\n",
    "                        'year': [2000, 2004, 2012],\n",
    "                        'rbi': [80, 100, 12]})\n",
    "                        \n",
    "print('{}\\n'.format(mlb_df1))\n",
    "print('{}\\n'.format(mlb_df1))\n",
    "\n",
    "mlb_merged = pd.merge(mlb_df1, mlb_df2)\n",
    "print('{}\\n'.format(mlb_merged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1    1\n",
      "r2    2\n",
      "Name: c1, dtype: int64\n",
      "\n",
      "    c1\n",
      "r1   1\n",
      "r2   2\n",
      "\n",
      "    c2  c3\n",
      "r1   3   5\n",
      "r2   4   6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'c1': [1, 2], 'c2': [3, 4],\n",
    "                   'c3': [5, 6]}, index=['r1', 'r2'])\n",
    "col1 = df['c1']\n",
    "# Newline for separating print statements\n",
    "print('{}\\n'.format(col1))\n",
    "\n",
    "col1_df = df[['c1']]\n",
    "print('{}\\n'.format(col1_df))\n",
    "\n",
    "col23 = df[['c2', 'c3']]\n",
    "print('{}\\n'.format(col23))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we use a single column label inside the bracket (as was the case for col1 in the code example), the output is a Series representing the corresponding column. When we use a list of column labels (as was the case for col1_df and col23), the output is a DataFrame that contains the corresponding columns.\n",
    "\n",
    "We can also use direct indexing to retrieve a subset of the rows (as a DataFrame). However, we can only retrieve rows based on slices, rather than specifying particular rows.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1  c2  c3\n",
      "r1   1   4   7\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n",
      "    c1  c2  c3\n",
      "r1   1   4   7\n",
      "r2   2   5   8\n",
      "\n",
      "    c1  c2  c3\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'c1': [1, 2, 3], 'c2': [4, 5, 6],\n",
    "                   'c3': [7, 8, 9]}, index=['r1', 'r2', 'r3'])\n",
    "\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "first_two_rows = df[0:2]\n",
    "print('{}\\n'.format(first_two_rows))\n",
    "\n",
    "last_two_rows = df['r2':'r3']\n",
    "print('{}\\n'.format(last_two_rows))\n",
    "\n",
    "# Results in KeyError\n",
    "#df['r1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that when we used integer indexing for the rows, the end index was exclusive (e.g. first_two_rows excluded the row at index 2). However, when we use row labels, the end index is inclusive (e.g. last_two_rows included the row labeled 'r3').\n",
    "\n",
    "Furthermore, when we tried to retrieve a single row based on its label, we received a KeyError. This is because the DataFrame treated 'r1' as a column label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from direct indexing, a DataFrame object also contains the loc and iloc properties for indexing.\n",
    "\n",
    "We use iloc to access rows based on their integer index. Using iloc we can access a single row as a Series, and specify particular rows to access through a list of integers or a boolean array.\n",
    "\n",
    "The code below shows how to use iloc to access a DataFrame's rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1  c2  c3\n",
      "r1   1   4   7\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n",
      "c1    2\n",
      "c2    5\n",
      "c3    8\n",
      "Name: r2, dtype: int64\n",
      "\n",
      "    c1  c2  c3\n",
      "r1   1   4   7\n",
      "r3   3   6   9\n",
      "\n",
      "    c1  c2  c3\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{}\\n'.format(df))\n",
    "\n",
    "print('{}\\n'.format(df.iloc[1]))\n",
    "\n",
    "print('{}\\n'.format(df.iloc[[0, 2]]))\n",
    "\n",
    "bool_list = [False, True, True]\n",
    "print('{}\\n'.format(df.iloc[bool_list]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loc property provides the same row indexing functionality as iloc, but uses row labels rather than integer indexes. Furthermore, with loc we can perform column indexing along with row indexing, and set new values in a DataFrame for specific rows and columns.\n",
    "\n",
    "The code below shows example usages of loc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1  c2  c3\n",
      "r1   1   4   7\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n",
      "c1    2\n",
      "c2    5\n",
      "c3    8\n",
      "Name: r2, dtype: int64\n",
      "\n",
      "    c1  c2  c3\n",
      "r2   2   5   8\n",
      "r3   3   6   9\n",
      "\n",
      "Single val: 4\n",
      "\n",
      "r1    4\n",
      "r3    6\n",
      "Name: c2, dtype: int64\n",
      "\n",
      "    c1  c2  c3\n",
      "r1   1   0   7\n",
      "r2   2   5   8\n",
      "r3   3   0   9\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "r1    0\n",
      "r2    5\n",
      "Name: c2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'c1': [1, 2, 3], 'c2': [4, 5, 6],\n",
    "                   'c3': [7, 8, 9]}, index=['r1', 'r2', 'r3'])\n",
    "                   \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "print('{}\\n'.format(df.loc['r2']))\n",
    "\n",
    "bool_list = [False, True, True]\n",
    "print('{}\\n'.format(df.loc[bool_list]))\n",
    "\n",
    "single_val = df.loc['r1', 'c2']\n",
    "print('Single val: {}\\n'.format(single_val))\n",
    "\n",
    "print('{}\\n'.format(df.loc[['r1', 'r3'], 'c2']))\n",
    "\n",
    "df.loc[['r1', 'r3'], 'c2'] = 0\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "\n",
    "print(df.iloc[0]['c1'])\n",
    "print(df.iloc[0, 0])\n",
    "print(df.loc['r1', 'c1'])\n",
    "print(df.loc[['r1', 'r2'], 'c2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the way we access rows and columns together with loc is similar to how we access 2-D NumPy arrays.\n",
    "\n",
    "Since we can't access columns on their own with loc or iloc, we still use bracket indexing when retrieving columns of a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File I/O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_df = pd.read_csv('stats.csv')\n",
    "# salary_df = pd.read_csv('salary.csv')\n",
    "# df = pd.merge(stats_df, salary_df)\n",
    "# df.to_csv('out.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('{}\\n'.format(df))\\n\\ngroups = df.groupby('yearID')\\nfor name, group in groups:\\n    print('Year: {}'.format(name))\\n    print('{}\\n'.format(group))\\n\\nprint('{}\\n'.format(groups.get_group(2016)))\\nprint('{}\\n'.format(groups.sum()))\\nprint('{}\\n'.format(groups.mean()))\\n\\nno2015 = groups.filter(lambda x: x.name > 2015)\\nprint(no2015)\\n\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "groups = df.groupby('yearID')\n",
    "for name, group in groups:\n",
    "    print('Year: {}'.format(name))\n",
    "    print('{}\\n'.format(group))\n",
    "\n",
    "print('{}\\n'.format(groups.get_group(2016)))\n",
    "print('{}\\n'.format(groups.sum()))\n",
    "print('{}\\n'.format(groups.mean()))\n",
    "\n",
    "no2015 = groups.filter(lambda x: x.name > 2015)\n",
    "print(no2015)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T1  T2  T3\n",
      "0  10  25  16\n",
      "1  15  27  15\n",
      "2   8  25  10\n",
      "\n",
      "T1    33\n",
      "T2    77\n",
      "T3    41\n",
      "dtype: int64\n",
      "\n",
      "0    51\n",
      "1    57\n",
      "2    43\n",
      "dtype: int64\n",
      "\n",
      "T1    11.000000\n",
      "T2    25.666667\n",
      "T3    13.666667\n",
      "dtype: float64\n",
      "\n",
      "0    17.000000\n",
      "1    19.000000\n",
      "2    14.333333\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'T1': [10, 15, 8],\n",
    "  'T2': [25, 27, 25],\n",
    "  'T3': [16, 15, 10]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "print('{}\\n'.format(df.sum()))\n",
    "\n",
    "print('{}\\n'.format(df.sum(axis=1)))\n",
    "\n",
    "print('{}\\n'.format(df.mean()))\n",
    "\n",
    "print('{}\\n'.format(df.mean(axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither function takes in a required argument. The most commonly used keyword argument for both functions is axis. The axis argument specifies whether to aggregate over rows (axis=0, the default), or columns (axis=1).\n",
    "\n",
    "In the code example, we used a DataFrame representing speed tests for three different processors (measured in seconds). When we used no argument, equivalent to using axis=0, the sum and mean functions calculated total and average times for each test. When we used axis=1, the sum and mean functions calculated total and average test times (across all three tests) for each processor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with aggregating quantitative features, we can also apply weights to them. We do this through the multiply function.\n",
    "\n",
    "The multiply function takes in a list of weights or a constant as its required argument. If a constant is used, the constant is multiplied across all the rows or columns (depending on the value of axis). If a list is used, then the position of each weight in the list corresponds to which row/column it is multiplied to.\n",
    "\n",
    "In contrast with sum and mean, the default axis for multiply is the columns axis. Therefore, to multiply weights along the rows of a DataFrame, we need to explicitly set axis=0.\n",
    "\n",
    "The code below shows example usages of multiply. The df DataFrame represents three different speed tests (columns) for two different processors (rows).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      T1      T2      T3\n",
      "0    0.1    0.25    0.16\n",
      "1  150.0  240.00  100.00\n",
      "\n",
      "      T1     T2      T3\n",
      "0    0.2    0.5    0.32\n",
      "1  300.0  480.0  200.00\n",
      "\n",
      "      T1     T2     T3\n",
      "0  100.0  250.0  160.0\n",
      "1  150.0  240.0  100.0\n",
      "\n",
      "      T1     T2     T3\n",
      "0  100.0  125.0  160.0\n",
      "1  150.0  120.0  100.0\n",
      "\n",
      "0    385.0\n",
      "1    370.0\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'T1': [0.1, 150.],\n",
    "  'T2': [0.25, 240.],\n",
    "  'T3': [0.16, 100.]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "print('{}\\n'.format(df.multiply(2)))\n",
    "\n",
    "df_ms = df.multiply([1000, 1], axis=0)\n",
    "print('{}\\n'.format(df_ms))\n",
    "\n",
    "df_w = df_ms.multiply([1,0.5,1])\n",
    "print('{}\\n'.format(df_w))\n",
    "print('{}\\n'.format(df_w.sum(axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the test times for processor 'p1' were measured in seconds, while the times for 'p2' were in milliseconds. So we made all the times in milliseconds by multiplying the values of 'p1' by 1000.\n",
    "\n",
    "Then we multiplied the values in 'T2' by 0.5, since those tests were done with two processors rather than one. This makes the final sum a weighted sum across the three columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4   cruzne02    2017    SEA  39\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: playerID, dtype: bool\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4    False\n",
      "Name: HR, dtype: bool\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: teamID, dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'playerID': ['bettsmo01', 'canoro01', 'cruzne02', 'ortizda01', 'cruzne02'],\n",
    "  'yearID': [2016, 2016, 2016, 2016, 2017],\n",
    "  'teamID': ['BOS', 'SEA', 'SEA', 'BOS', 'SEA'],\n",
    "  'HR': [31, 39, 43, 38, 39]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "cruzne02 = df['playerID'] == 'cruzne02'\n",
    "print('{}\\n'.format(cruzne02))\n",
    "\n",
    "hr40 = df['HR'] > 40\n",
    "print('{}\\n'.format(hr40))\n",
    "\n",
    "notbos = df['teamID'] != 'BOS'\n",
    "print('{}\\n'.format(notbos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4   cruzne02    2017    SEA  39\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: playerID, dtype: bool\n",
      "\n",
      "0     True\n",
      "1    False\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "Name: teamID, dtype: bool\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: playerID, dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'playerID': ['bettsmo01', 'canoro01', 'cruzne02', 'ortizda01', 'cruzne02'],\n",
    "  'yearID': [2016, 2016, 2016, 2016, 2017],\n",
    "  'teamID': ['BOS', 'SEA', 'SEA', 'BOS', 'SEA'],\n",
    "  'HR': [31, 39, 43, 38, 39]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "str_f1 = df['playerID'].str.startswith('c')\n",
    "print('{}\\n'.format(str_f1))\n",
    "\n",
    "str_f2 = df['teamID'].str.endswith('S')\n",
    "print('{}\\n'.format(str_f2))\n",
    "\n",
    "str_f3 = ~df['playerID'].str.contains('o')\n",
    "print('{}\\n'.format(str_f3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4   cruzne02    2017    SEA  39\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3     True\n",
      "4     True\n",
      "Name: playerID, dtype: bool\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4     True\n",
      "Name: yearID, dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'playerID': ['bettsmo01', 'canoro01', 'cruzne02', 'ortizda01', 'cruzne02'],\n",
    "  'yearID': [2016, 2016, 2016, 2016, 2017],\n",
    "  'teamID': ['BOS', 'SEA', 'SEA', 'BOS', 'SEA'],\n",
    "  'HR': [31, 39, 43, 38, 39]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "isin_f1 = df['playerID'].isin(['cruzne02',\n",
    "                               'ortizda01'])\n",
    "print('{}\\n'.format(isin_f1))\n",
    "\n",
    "isin_f2 = df['yearID'].isin([2015, 2017])\n",
    "print('{}\\n'.format(isin_f2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2    doejo01    2017    NaN  99\n",
      "\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "Name: teamID, dtype: bool\n",
      "\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "Name: teamID, dtype: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'playerID': ['bettsmo01', 'canoro01', 'doejo01'],\n",
    "  'yearID': [2016, 2016, 2017],\n",
    "  'teamID': ['BOS', 'SEA', np.nan],\n",
    "  'HR': [31, 39, 99]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "isna = df['teamID'].isna()\n",
    "print('{}\\n'.format(isna))\n",
    "\n",
    "notna = df['teamID'].notna()\n",
    "print('{}\\n'.format(notna))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n",
      "   playerID  yearID teamID  HR\n",
      "2  cruzne02    2016    SEA  43\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "  'playerID': ['bettsmo01', 'canoro01', 'cruzne02', 'ortizda01', 'bettsmo01'],\n",
    "  'yearID': [2016, 2016, 2016, 2016, 2015],\n",
    "  'teamID': ['BOS', 'SEA', 'SEA', 'BOS', 'BOS'],\n",
    "  'HR': [31, 39, 43, 38, 18]})\n",
    "  \n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "hr40_df = df[df['HR'] > 40]\n",
    "print('{}\\n'.format(hr40_df))\n",
    "\n",
    "not_hr30_df = df[~(df['HR'] > 30)]\n",
    "print('{}\\n'.format(not_hr30_df))\n",
    "\n",
    "str_df = df[df['teamID'].str.startswith('B')]\n",
    "print('{}\\n'.format(str_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "4  bettsmo01    2015    BOS  18\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "3  ortizda01    2016    BOS  38\n",
      "2   cruzne02    2016    SEA  43\n",
      "1   canoro01    2016    SEA  39\n",
      "0  bettsmo01    2016    BOS  31\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df is predefined\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "sort1 = df.sort_values('yearID')\n",
    "print('{}\\n'.format(sort1))\n",
    "\n",
    "sort2 = df.sort_values('playerID', ascending=False)\n",
    "print('{}\\n'.format(sort2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "4  bettsmo01    2015    BOS  18\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "\n",
      "    playerID  yearID teamID  HR\n",
      "4  bettsmo01    2015    BOS  18\n",
      "2   cruzne02    2016    SEA  43\n",
      "1   canoro01    2016    SEA  39\n",
      "3  ortizda01    2016    BOS  38\n",
      "0  bettsmo01    2016    BOS  31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df is predefined\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "sort1 = df.sort_values(['yearID', 'playerID'])\n",
    "print('{}\\n'.format(sort1))\n",
    "\n",
    "sort2 = df.sort_values(['yearID', 'HR'],\n",
    "                       ascending=[True, False])\n",
    "print('{}\\n'.format(sort2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n",
      "            yearID         HR\n",
      "count     5.000000   5.000000\n",
      "mean   2015.800000  33.800000\n",
      "std       0.447214   9.833616\n",
      "min    2015.000000  18.000000\n",
      "25%    2016.000000  31.000000\n",
      "50%    2016.000000  38.000000\n",
      "75%    2016.000000  39.000000\n",
      "max    2016.000000  43.000000\n",
      "\n",
      "              HR\n",
      "count   5.000000\n",
      "mean   33.800000\n",
      "std     9.833616\n",
      "min    18.000000\n",
      "25%    31.000000\n",
      "50%    38.000000\n",
      "75%    39.000000\n",
      "max    43.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df is predefined\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "metrics1 = df.describe()\n",
    "print('{}\\n'.format(metrics1))\n",
    "\n",
    "hr_rbi = df[['HR']]\n",
    "metrics2 = hr_rbi.describe()\n",
    "print('{}\\n'.format(metrics2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using describe with a DataFrame will return a summary of metrics for each of the DataFrame's numeric features. In our example, df had three features with numerical values: yearID, HR, and RBI.\n",
    "\n",
    "Since we normally treat yearID as a categorical feature, the second time we used describe was with the hr_rbi DataFrame, which only included the HR and RBI features.\n",
    "\n",
    "To have describe return specific percentiles, we can use the percentiles keyword argument. The percentiles argument takes in a list of decimal percentages, representing the percentiles we want returned in the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              HR\n",
      "count   5.000000\n",
      "mean   33.800000\n",
      "std     9.833616\n",
      "min    18.000000\n",
      "50%    38.000000\n",
      "max    43.000000\n",
      "\n",
      "              HR\n",
      "count   5.000000\n",
      "mean   33.800000\n",
      "std     9.833616\n",
      "min    18.000000\n",
      "10%    23.200000\n",
      "50%    38.000000\n",
      "max    43.000000\n",
      "\n",
      "              HR\n",
      "count   5.000000\n",
      "mean   33.800000\n",
      "std     9.833616\n",
      "min    18.000000\n",
      "20%    28.400000\n",
      "50%    38.000000\n",
      "80%    39.800000\n",
      "max    43.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics1 = hr_rbi.describe(percentiles=[.5])\n",
    "print('{}\\n'.format(metrics1))\n",
    "\n",
    "metrics2 = hr_rbi.describe(percentiles=[.1])\n",
    "print('{}\\n'.format(metrics2))\n",
    "\n",
    "metrics3 = hr_rbi.describe(percentiles=[.2,.8])\n",
    "print('{}\\n'.format(metrics3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bettsmo01    2\n",
      "canoro01     1\n",
      "ortizda01    1\n",
      "cruzne02     1\n",
      "Name: playerID, dtype: int64\n",
      "\n",
      "bettsmo01    0.4\n",
      "canoro01     0.2\n",
      "ortizda01    0.2\n",
      "cruzne02     0.2\n",
      "Name: playerID, dtype: float64\n",
      "\n",
      "cruzne02     1\n",
      "ortizda01    1\n",
      "canoro01     1\n",
      "bettsmo01    2\n",
      "Name: playerID, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_ids = df['playerID']\n",
    "print('{}\\n'.format(p_ids.value_counts()))\n",
    "\n",
    "print('{}\\n'.format(p_ids.value_counts(normalize=True)))\n",
    "\n",
    "print('{}\\n'.format(p_ids.value_counts(ascending=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['bettsmo01', 'canoro01', 'cruzne02', 'ortizda01'], dtype=object)\n",
      "\n",
      "array(['BOS', 'SEA'], dtype=object)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_players = df['playerID'].unique()\n",
    "print('{}\\n'.format(repr(unique_players)))\n",
    "\n",
    "unique_teams = df['teamID'].unique()\n",
    "print('{}\\n'.format(repr(unique_teams)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2016\n",
      "1    2016\n",
      "2    2016\n",
      "3    2016\n",
      "4    2015\n",
      "Name: yearID, dtype: int64\n",
      "\n",
      "array([2016, 2015])\n",
      "\n",
      "2016    4\n",
      "2015    1\n",
      "Name: yearID, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_ids = df['yearID']\n",
    "print('{}\\n'.format(y_ids))\n",
    "\n",
    "print('{}\\n'.format(repr(y_ids.unique())))\n",
    "print('{}\\n'.format(y_ids.value_counts()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    playerID  yearID teamID  HR\n",
      "0  bettsmo01    2016    BOS  31\n",
      "1   canoro01    2016    SEA  39\n",
      "2   cruzne02    2016    SEA  43\n",
      "3  ortizda01    2016    BOS  38\n",
      "4  bettsmo01    2015    BOS  18\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predefined df\n",
    "print('{}\\n'.format(df))\n",
    "\n",
    "df.plot(x='yearID', y='HR')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZ2UlEQVR4nO3df5BV5Z3n8feH5vciJUJrCA3bJGmtoGZAbwgpKy6uM4FxZxR0M4tVG92JqTZGa4lOaiNJjJoqU9buqDvubrQwodBKAiEbHdiUv1kda7c02CgRAR0QSWgh0AFXwR+tdH/3j3taD923u++vvreb83lV3ernfs9zzvn2Q/O9p597+j6KCMzMLBtG1TsBMzOrHRd9M7MMcdE3M8sQF30zswxx0Tczy5DR9U5gMNOmTYvm5uZ6p2FmNqJs3rz5TxHR2Ds+7It+c3MzbW1t9U7DzGxEkfT7QnFP75iZZYiLvplZhrjom5lliIu+mVmGuOibmWWIi76ZWYa46JuZZcig9+lLmgk8AHwC6AZWRsQ/SDoF+CXQDOwB/iYi3kz2WQFcBXQB/zEiHkvi5wKrgQnAw8DyGKLPdl79f1/n8DsfDMWhzawapI+bhcMo2XJ8rJ++6SeDHK/oY/bT//jjF/N99NO/nz49Gy7//ExGN1T32ryYP846BvxdRLwg6SRgs6QngP8AbIyI2yXdCNwIfEfSHGAZcCbwSeBJSadHRBdwD9AKPEe+6C8GHqnqd5T4xaY/sPPg0aE4tJlVyMt4FOcr5zYxuqG6xxy06EfEfmB/0j4iaQcwA7gEWJh0ux94GvhOEl8bEZ3A65J2AfMl7QEmR8SzAJIeAJYwREX/8ev/1VAc1syGUPoX/55m9Lf9uHiqTd9j9D3P4P1LPS9DcMxxo6s/A1/SxzBIagbmAb8FTkteEIiI/ZJOTbrNIH8l36M9iX2YtHvHC52nlfxvBMyaNauUFM1sBOtv6iPVo2a5nKiKfhmRNAn4NfCtiHh7oK4FYjFAvG8wYmVE5CIi19jY5/OCzMysTEUVfUljyBf8n0fEg0n4gKTpyfbpwMEk3g7MTO3eBOxL4k0F4mZmViODFn3lf9/6KbAjIu5MbdoAXJm0rwTWp+LLJI2TNBtoATYlU0FHJC1IjnlFah8zM6uBYub0zwO+CmyVtCWJfRe4HVgn6SrgD8BXACJim6R1wHbyd/5cm9y5A3ANH9+y+QhD9CaumZkVpiG6Tb5qcrlc+PP0zcxKI2lzROR6x/0XuWZmGeKib2aWIS76ZmYZ4qJvZpYhLvpmZhniom9mliEu+mZmGeKib2aWIS76ZmYZ4qJvZpYhLvpmZhniom9mliEu+mZmGeKib2aWIS76ZmYZUszKWaskHZT0cir2S0lbkseensVVJDVLei+17d7UPudK2ippl6S7pcLLHpuZ2dApZuWs1cB/Bx7oCUTEv+tpS7oDeCvV/7WImFvgOPcArcBzwMPAYrxylplZTQ16pR8RzwCHC21Lrtb/Blgz0DGShdMnR8SzkV+q6wFgSenpmplZJSqd0/8ScCAidqZisyW9KOmfJH0pic0A2lN92pOYmZnVUDHTOwO5nOOv8vcDsyLikKRzgX+UdCZQaP6+38V5JbWSnwpi1qxZFaZoZmY9yr7SlzQauBT4ZU8sIjoj4lDS3gy8BpxO/sq+KbV7E7Cvv2NHxMqIyEVErrGxsdwUzcysl0qmd/4ceCUiPpq2kdQoqSFpfwpoAXZHxH7giKQFyfsAVwDrKzi3mZmVoZhbNtcAzwJnSGqXdFWyaRl938A9H3hJ0u+A/wl8IyJ63gS+BvgJsIv8bwC+c8fMrMaUv5lm+MrlctHW1lbvNMzMRhRJmyMi1zvuv8g1M8sQF30zswxx0TczyxAXfTOzDHHRNzPLEBd9M7MMcdE3M8sQF30zswxx0TczyxAXfTOzDHHRNzPLEBd9M7MMcdE3M8sQF30zswxx0TczyxAXfTOzDClm5axVkg5KejkVu0XSG5K2JI+LUttWSNol6VVJi1LxcyVtTbbdnSybaGZmNVTMlf5qYHGB+F0RMTd5PAwgaQ75ZRTPTPb5cc+aucA9QCv5dXNb+jmmmZkNoUGLfkQ8AxwerF/iEmBtRHRGxOvk18OdL2k6MDkino38+owPAEvKTdrMzMpTyZz+dZJeSqZ/piSxGcDeVJ/2JDYjafeOFySpVVKbpLaOjo4KUjQzs7Ryi/49wKeBucB+4I4kXmiePgaIFxQRKyMiFxG5xsbGMlM0M7Peyir6EXEgIroiohu4D5ifbGoHZqa6NgH7knhTgbiZmdVQWUU/maPvsRToubNnA7BM0jhJs8m/YbspIvYDRyQtSO7auQJYX0HeZmZWhtGDdZC0BlgITJPUDtwMLJQ0l/wUzR7gaoCI2CZpHbAdOAZcGxFdyaGuIX8n0ATgkeRhZmY1pPzNNMNXLpeLtra2eqdhZjaiSNocEbnecf9FrplZhrjom5lliIu+mVmGuOibmWWIi76ZWYa46JuZZYiLvplZhrjom5lliIu+mVmGuOibmWWIi76ZWYa46JuZZYiLvplZhrjom5lliIu+mVmGuOibmWXIoEVf0ipJByW9nIr9F0mvSHpJ0kOSTk7izZLek7Qledyb2udcSVsl7ZJ0d7JsopmZ1VAxV/qrgcW9Yk8AZ0XE54B/Blaktr0WEXOTxzdS8XuAVvLr5rYUOKaZmQ2xQYt+RDwDHO4VezwijiVPnwOaBjpGspD65Ih4NvLrMz4ALCkvZTMzK1c15vS/xvGLnM+W9KKkf5L0pSQ2A2hP9WlPYgVJapXUJqmto6OjCimamRlUWPQlfQ84Bvw8Ce0HZkXEPOAG4BeSJgOF5u/7XZE9IlZGRC4ico2NjZWkaGZmKaPL3VHSlcBfARcmUzZERCfQmbQ3S3oNOJ38lX16CqgJ2Ffuuc3MrDxlXelLWgx8B7g4It5NxRslNSTtT5F/w3Z3ROwHjkhakNy1cwWwvuLszcysJINe6UtaAywEpklqB24mf7fOOOCJ5M7L55I7dc4HfijpGNAFfCMiet4Evob8nUATyL8HkH4fwMzMakDJzMywlcvloq2trd5pmJmNKJI2R0Sud9x/kWtmliFlv5FrZlYPH374Ie3t7bz//vv1TmVYGD9+PE1NTYwZM6ao/i76ZjaitLe3c9JJJ9Hc3EzWP80lIjh06BDt7e3Mnj27qH08vWNmI8r777/P1KlTM1/wASQxderUkn7rcdE3sxHHBf9jpY6Fi76Z2TCwcOFCeu5UbG5u5uyzz+bss89mzpw5fP/736ezs7Mq53HRNzOrs66urj6xp556iq1bt7Jp0yZ2795Na2trVc7lN3LNzEpw0003MW3aNJYvXw7A9773PU477TQ6OztZt24dnZ2dLF26lFtvvRWAJUuWsHfvXt5//32WL1/+UfGeNGkSN9xwA4899hh33HFHv+ebNGkS9957LzNnzuTw4cOccsopFeXvom9mI9at/2sb2/e9XdVjzvnkZG7+6zP73X7VVVdx6aWXsnz5crq7u1m7di0/+tGP2LhxI5s2bSIiuPjii3nmmWc4//zzWbVqFaeccgrvvfcen//857nsssuYOnUq77zzDmeddRY//OEPB81p8uTJzJ49m507d/KFL3yhou/PRd/MrATNzc1MnTqVF198kQMHDjBv3jyef/55Hn/8cebNmwfA0aNH2blzJ+effz533303Dz30EAB79+5l586dTJ06lYaGBi677LKiz1utT09w0TezEWugK/Kh9PWvf53Vq1fzxz/+ka997Wts3LiRFStWcPXVVx/X7+mnn+bJJ5/k2WefZeLEiSxcuPCj2yvHjx9PQ0NDUec7cuQIe/bs4fTTT684d7+Ra2ZWoqVLl/Loo4/y/PPPs2jRIhYtWsSqVas4evQoAG+88QYHDx7krbfeYsqUKUycOJFXXnmF5557ruRzHT16lG9+85ssWbKEKVOmVJy7r/TNzEo0duxYLrjgAk4++WQaGhr48pe/zI4dO/jiF78I5N98/dnPfsbixYu59957+dznPscZZ5zBggULij7HBRdcQETQ3d3N0qVLuemmm6qSuz9l08xGlB07dvDZz362rjl0d3dzzjnn8Ktf/YqWlpa65gKFx8SfsmlmVgXbt2/nM5/5DBdeeOGwKPil8vSOmVkJ5syZw+7du+udRtkGvdKXtErSQUkvp2KnSHpC0s7k65TUthWSdkl6VdKiVPxcSVuTbXfLH55hZlZzxUzvrAYW94rdCGyMiBZgY/IcSXOAZcCZyT4/7lkzF7gHaCW/bm5LgWOamdkQG7ToR8QzwOFe4UuA+5P2/cCSVHxtRHRGxOvALmC+pOnA5Ih4NvLvHD+Q2sfMzGqk3DdyT4uI/QDJ11OT+Axgb6pfexKbkbR7xwuS1CqpTVJbR0dHmSmamVlv1b57p9A8fQwQLygiVkZELiJyjY2NVUvOzKwaJk2adNzz1atXc9111wFwyy23MGPGDObOncucOXNYs2ZNPVLsV7lF/0AyZUPy9WASbwdmpvo1AfuSeFOBuJnZCef6669ny5YtrF+/nquvvpoPP/yw3il9pNyivwG4MmlfCaxPxZdJGidpNvk3bDclU0BHJC1I7tq5IrWPmdkJqaWlhYkTJ/Lmm2/WO5WPDHqfvqQ1wEJgmqR24GbgdmCdpKuAPwBfAYiIbZLWAduBY8C1EdGzOsA15O8EmgA8kjzMzMr3yI3wx63VPeYnzoa/vH3ALu+99x5z58796Pnhw4e5+OKL+/R74YUXaGlp4dRTT+2zrV4GLfoRcXk/my7sp/9twG0F4m3AWSVlZ2Y2DE2YMIEtW7Z89Hz16tWkPy7mrrvu4r777mP37t08+uij9UixX/6LXDMbuQa5Iq+X66+/nm9/+9s8+OCDXHHFFbz22muMHz++3mkB/uwdM7Mhc+mll5LL5bj//vsH71wjLvpmZkPoBz/4AXfeeSfd3d31TgXwRyub2QgzHD5aebjxRyubmVlBLvpmZhniom9mliEu+mY24gz39yJrqdSxcNE3sxFl/PjxHDp0yIWffME/dOhQSX8D4D/OMrMRpampifb2dvyx63njx4+nqalp8I4JF30zG1HGjBnD7Nmz653GiOXpHTOzDHHRNzPLEBd9M7MMcdE3M8uQsou+pDMkbUk93pb0LUm3SHojFb8otc8KSbskvSppUXW+BTMzK1bZd+9ExKvAXABJDcAbwEPA3wJ3RcTfp/tLmgMsA84EPgk8Ken01MpaZmY2xKo1vXMh8FpE/H6APpcAayOiMyJeB3YB86t0fjMzK0K1iv4yYE3q+XWSXpK0StKUJDYD2Jvq057E+pDUKqlNUpv/AMPMrHoqLvqSxgIXA79KQvcAnyY/9bMfuKOna4HdC/4ddUSsjIhcROQaGxsrTdHMzBLVuNL/S+CFiDgAEBEHIqIrIrqB+/h4CqcdmJnarwnYV4Xzm5lZkapR9C8nNbUjaXpq21Lg5aS9AVgmaZyk2UALsKkK5zczsyJV9Nk7kiYCfwFcnQr/Z0lzyU/d7OnZFhHbJK0DtgPHgGt9546ZWW1VVPQj4l1gaq/YVwfofxtwWyXnNDOz8vkvcs3MMsRF38wsQ1z0zcwyxEXfzCxDXPTNzDLERd/MLENc9M3MMsRF38wsQ1z0zcwyxEXfzCxDXPTNzDLERd/MLENc9M3MMsRF38wsQ1z0zcwyxEXfzCxDKir6kvZI2ippi6S2JHaKpCck7Uy+Tkn1XyFpl6RXJS2qNHkzMytNNa70L4iIuRGRS57fCGyMiBZgY/IcSXOAZcCZwGLgx5IaqnB+MzMr0lBM71wC3J+07weWpOJrI6IzIl4HdgHzh+D8ZmbWj0qLfgCPS9osqTWJnRYR+wGSr6cm8RnA3tS+7UmsD0mtktoktXV0dFSYopmZ9ahoYXTgvIjYJ+lU4AlJrwzQVwViUahjRKwEVgLkcrmCfczMrHQVXelHxL7k60HgIfLTNQckTQdIvh5MurcDM1O7NwH7Kjm/mZmVpuyiL+lfSDqppw18GXgZ2ABcmXS7EliftDcAyySNkzQbaAE2lXt+MzMrXSXTO6cBD0nqOc4vIuJRSc8D6yRdBfwB+ApARGyTtA7YDhwDro2IroqyNzOzkpRd9CNiN/BnBeKHgAv72ec24LZyz2lmZpXxX+SamWWIi76ZWYa46JuZZYiLvplZhrjom5lliIu+mVmGuOibmWWIi76ZWYa46JuZZYiLvplZhrjom5lliIu+mVmGuOibmWWIi76ZWYa46JuZZYiLvplZhlSyXOJMSU9J2iFpm6TlSfwWSW9I2pI8Lkrts0LSLkmvSlpUjW/AzMyKV8lyiceAv4uIF5K1cjdLeiLZdldE/H26s6Q5wDLgTOCTwJOSTveSiWZmtVP2lX5E7I+IF5L2EWAHMGOAXS4B1kZEZ0S8DuwC5pd7fjMzK11V5vQlNQPzgN8moeskvSRplaQpSWwGsDe1Wzv9vEhIapXUJqmto6OjGimamRlVKPqSJgG/Br4VEW8D9wCfBuYC+4E7eroW2D0KHTMiVkZELiJyjY2NlaZoZmaJioq+pDHkC/7PI+JBgIg4EBFdEdEN3MfHUzjtwMzU7k3AvkrOb2Zmpank7h0BPwV2RMSdqfj0VLelwMtJewOwTNI4SbOBFmBTuec3M7PSVXL3znnAV4GtkrYkse8Cl0uaS37qZg9wNUBEbJO0DthO/s6fa33njplZbZVd9CPi/1B4nv7hAfa5Dbit3HOamVll/Be5ZmYZ4qJvZpYhLvpmZhniom9mliEu+mZmGeKib2aWIS76ZmYZ4qJvZpYhLvpmZhniom9mliEu+mZmGeKib2aWIS76ZmYZUslHK5v1FQHRnf9Kse0osX9Pm8Lx6E4dt7vAOQbqU6hN4TiARgHKf9Wo/OfO9rSPi6tAXP3Ee/dnkOOk4yrivKlzW+acuEX/7f3Q1QndXUkx6E7aXal2f/Gu/H/uAft357cd1+4uMZ6cZ6B4SXlXEu/++PFRASXVLqIQ28hTtRenIl5sRo2ChnHQMBZGj03aY2D0uF7tsUmf/tpjCxyniGP6RQ44kYv+A5fAn16tdxZ9aRSoIflP0JBq944nz0eNSrWLiPf8kBfb/7hz9/6Pql5t9ROvVv+eq9pS+vc+V+8C1F+bEvur77mA414g+7xopn7zOC4e/cR792eQ46TjUcR5S8kzij9vsd9zd1f+QqzrQ/jgXeh6M98+1gldH+Qf6Xb3ser+3xvV+0WlhBeMar3wFHoxG9VQ3e9zEDUv+pIWA/8ANAA/iYjbh+REF94EnUfKL56jeopgoQLd06+UeEOqYJjZoLq7Ui8GHyQvGOl2zwtG58AvHse1izjOu+8Mvm81qaH/F4/Wp2DMhKqerqZFX1ID8D+AvyC/UPrzkjZExPaqn+yzf131Q5pZDY1qgFETql70KhaRf3Eo6sUm6ddvO+l7XDt1nFFjqp5+ra/05wO7ImI3gKS1wCXk1801Mxv+pPyV+Oix9c6kLLW+ZXMGsDf1vD2JHUdSq6Q2SW0dHR01S87M7ERX66JfaEI7+gQiVkZELiJyjY2NNUjLzCwbal3024GZqedNwL4a52Bmllm1LvrPAy2SZksaCywDNtQ4BzOzzKrpG7kRcUzSdcBj5G/ZXBUR22qZg5lZltX8Pv2IeBh4uNbnNTMzf+CamVmmuOibmWWIIvrcMTmsSOoAfl/m7tOAP1UxnWpxXqVxXqVxXqU5UfP6lxHR5573YV/0KyGpLSJy9c6jN+dVGudVGudVmqzl5ekdM7MMcdE3M8uQE73or6x3Av1wXqVxXqVxXqXJVF4n9Jy+mZkd70S/0jczsxQXfTOzDDkhir6kxZJelbRL0o0FtkvS3cn2lySdM0zyWijpLUlbkscPapDTKkkHJb3cz/Z6jdVgedV8rJLzzpT0lKQdkrZJWl6gT83HrMi86vHzNV7SJkm/S/K6tUCfeoxXMXnV5WcsOXeDpBcl/abAtuqOV0SM6Af5D257DfgUMBb4HTCnV5+LgEfIf57/AuC3wySvhcBvajxe5wPnAC/3s73mY1VkXjUfq+S804FzkvZJwD8Pk5+vYvKqx8+XgElJewzwW2DBMBivYvKqy89Ycu4bgF8UOn+1x+tEuNL/aAnGiPgA6FmCMe0S4IHIew44WdL0YZBXzUXEM8DhAbrUY6yKyasuImJ/RLyQtI8AO+i72lvNx6zIvGouGYOjydMxyaP33SL1GK9i8qoLSU3AvwF+0k+Xqo7XiVD0i1mCsahlGuuQF8AXk185H5F05hDnVIx6jFWx6jpWkpqBeeSvEtPqOmYD5AV1GLNkqmILcBB4IiKGxXgVkRfU52fsvwL/CejuZ3tVx+tEKPrFLMFY1DKNVVbMOV8g//kYfwb8N+AfhzinYtRjrIpR17GSNAn4NfCtiHi79+YCu9RkzAbJqy5jFhFdETGX/Mp48yWd1atLXcariLxqPl6S/go4GBGbB+pWIFb2eJ0IRb+YJRjrsUzjoOeMiLd7fuWM/DoDYyRNG+K8BjMsl7Ss51hJGkO+sP48Ih4s0KUuYzZYXvX++YqI/wc8DSzutamuP2P95VWn8ToPuFjSHvJTwP9a0s969anqeJ0IRb+YJRg3AFck74IvAN6KiP31zkvSJyQpac8n/+9xaIjzGkw9xmpQ9Rqr5Jw/BXZExJ39dKv5mBWTVz3GTFKjpJOT9gTgz4FXenWrx3gNmlc9xisiVkREU0Q0k68R/zsi/n2vblUdr5qvnFVt0c8SjJK+kWy/l/xKXRcBu4B3gb8dJnn9W+AaSceA94BlkbxdP1QkrSF/l8I0Se3AzeTf1KrbWBWZV83HKnEe8FVgazIfDPBdYFYqt3qMWTF51WPMpgP3S2ogXzTXRcRv6v3/sci86vUz1sdQjpc/hsHMLENOhOkdMzMrkou+mVmGuOibmWWIi76ZWYa46JuZZYiLvplZhrjom5llyP8HWp9Kgc917doAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()\n",
    "df.plot(kind='hist')\n",
    "df.plot(kind='box')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most scikit-learn functions, the input data comes in the form of a NumPy array.\n",
    "\n",
    "Note: The array’s rows represent individual data observations, while each column represents a particular feature of the data, i.e. the same format as a spreadsheet data table.\n",
    "\n",
    "The scikit-learn data preprocessing module is called sklearn.preprocessing. One of the functions in this module, scale, applies data standardization to a given axis of a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[-0.16552118, -1.06904497, -0.1393466 ],\n",
      "       [ 1.4896906 ,  0.26726124,  1.60248593],\n",
      "       [-1.40693001, -1.06904497, -1.53281263],\n",
      "       [-0.57932412,  1.60356745, -0.1393466 ],\n",
      "       [ 0.66208471,  0.26726124,  0.2090199 ]])\n",
      "\n",
      "array([ 0., -0.,  0.])\n",
      "\n",
      "array([1., 1., 1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pizza_data = np.array([[2100,   10,  800],\n",
    "                       [2500,   11,  850],\n",
    "                       [1800,   10,  760],\n",
    "                       [2000,   12,  800],\n",
    "                       [2300,   11,  810]])\n",
    "\n",
    "\n",
    "# predefined pizza data\n",
    "# Newline to separate print statements\n",
    "print('{}\\n'.format(repr(pizza_data)))\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "# Standardizing each column of pizza_data\n",
    "col_standardized = scale(pizza_data)\n",
    "print('{}\\n'.format(repr(col_standardized)))\n",
    "\n",
    "# Column means (rounded to nearest thousandth)\n",
    "col_means = col_standardized.mean(axis=0).round(decimals=3)\n",
    "print('{}\\n'.format(repr(col_means)))\n",
    "\n",
    "# Column standard deviations\n",
    "col_stds = col_standardized.std(axis=0)\n",
    "print('{}\\n'.format(repr(col_stds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[0.42857143, 0.        , 0.44444444],\n",
      "       [1.        , 0.5       , 1.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.28571429, 1.        , 0.44444444],\n",
      "       [0.71428571, 0.5       , 0.55555556]])\n",
      "\n",
      "array([[ 0.14285714, -2.        ,  0.22222222],\n",
      "       [ 3.        ,  0.5       ,  3.        ],\n",
      "       [-2.        , -2.        , -2.        ],\n",
      "       [-0.57142857,  3.        ,  0.22222222],\n",
      "       [ 1.57142857,  0.5       ,  0.77777778]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pizza_data\n",
    "\n",
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "default_scaler = MinMaxScaler() # the default range is [0,1]\n",
    "transformed = default_scaler.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n",
    "\n",
    "custom_scaler = MinMaxScaler(feature_range=(-2, 3))\n",
    "transformed = custom_scaler.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data scaling methods from the previous two chapters are both affected by outliers. Data standardization uses each feature's mean and standard deviation, while ranged scaling uses the maximum and minimum feature values, meaning that they're both susceptible to being skewed by outlier values.\n",
    "\n",
    "We can robustly scale the data, i.e. avoid being affected by outliers, by using use the data's median and Interquartile Range (IQR). Since the median and IQR are percentile measurements of the data (50% for median, 25% to 75% for the IQR), they are not affected by outliers. For the scaling method, we just subtract the median from each data value then scale to the IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[ 0.        , -1.        ,  0.        ],\n",
      "       [ 1.33333333,  0.        ,  5.        ],\n",
      "       [-1.        , -1.        , -4.        ],\n",
      "       [-0.33333333,  1.        ,  0.        ],\n",
      "       [ 0.66666667,  0.        ,  1.        ]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "transformed = robust_scaler.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[0.93447848, 0.0044499 , 0.3559918 ],\n",
      "       [0.94676453, 0.00416576, 0.32189994],\n",
      "       [0.9212375 , 0.00511799, 0.38896694],\n",
      "       [0.92846228, 0.00557077, 0.37138491],\n",
      "       [0.94320765, 0.00451099, 0.33217313]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "transformed = normalizer.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, we often have to deal with data that contains missing values. Sometimes, if the dataset is missing too many values, we just don't use it. However, if only a few of the values are missing, we can perform data imputation to substitute the missing data with some other value(s).\n",
    "\n",
    "There are many different methods for data imputation. In scikit-learn, the SimpleImputer transformer performs four different data imputation methods.\n",
    "\n",
    "The four methods are:\n",
    "\n",
    "- Using the mean value\n",
    "- Using the median value\n",
    "- Using the most frequent value\n",
    "- Filling in missing values with a constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[2100.,   10.,  800.],\n",
      "       [2500.,   11.,  850.],\n",
      "       [1800.,   10.,  760.],\n",
      "       [2000.,   12.,  800.],\n",
      "       [2300.,   11.,  810.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer()\n",
    "transformed = imp_mean.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy arrays, missing data is represented by the np.nan value. In the above example, we replaced each missing value with the mean of the values in its column.\n",
    "\n",
    "The default imputation method for SimpleImputer is using the column means. By using the strategy keyword argument when initializing a SimpleImputer object, we can specify a different imputation method.\n",
    "\n",
    "The code below demonstrates various initialization strategies for SimpleImputer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[2100.,   10.,  800.],\n",
      "       [2500.,   11.,  850.],\n",
      "       [1800.,   10.,  760.],\n",
      "       [2000.,   12.,  800.],\n",
      "       [2300.,   11.,  810.]])\n",
      "\n",
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_median = SimpleImputer(strategy='median')\n",
    "transformed = imp_median.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n",
    "\n",
    "imp_frequent = SimpleImputer(strategy='most_frequent')\n",
    "transformed = imp_frequent.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'median' strategy fills in missing data with the median from each column, while the 'most_frequent' strategy uses the value that appears the most for each column.\n",
    "\n",
    "The final imputation method that SimpleImputer provides is to fill in missing values with a specified constant. This can be useful if there is already a suitable substitute for missing data (e.g. 0 or -1).\n",
    "\n",
    "The code below demonstrates how to fill in missing data with a specific constant. The fill_value keyword argument is used when initializing the SimpleImputer object, to specify the constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_constant = SimpleImputer(strategy='constant',\n",
    "                             fill_value=-1)\n",
    "transformed = imp_constant.fit_transform(data)\n",
    "print('{}\\n'.format(repr(transformed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer object only implements the four imputation methods shown in section A. However, data imputation is not limited to those four methods.\n",
    "\n",
    "There are also more advanced imputation methods such as k-Nearest Neighbors (filling in missing values based on similarity scores from the kNN algorithm) and MICE (applying multiple chained imputations, assuming the missing values are randomly distributed across observations).\n",
    "\n",
    "In most industry cases these advanced methods are not required, since the data is either perfectly cleaned or the missing values are scarce. Nevertheless, the advanced methods could be useful when dealing with open source datasets, since these tend to be more incomplete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,   10,  800],\n",
      "       [2500,   11,  850],\n",
      "       [1800,   10,  760],\n",
      "       [2000,   12,  800],\n",
      "       [2300,   11,  810]])\n",
      "\n",
      "array([[-4.01960e+01, -5.12000e-01,  7.93000e-01],\n",
      "       [ 3.62893e+02, -4.97800e+00,  3.42000e-01],\n",
      "       [-3.42795e+02,  5.28200e+00,  2.60000e-01],\n",
      "       [-1.39553e+02, -1.19100e+01, -7.34000e-01],\n",
      "       [ 1.59652e+02,  1.21170e+01, -6.61000e-01]])\n",
      "\n",
      "array([[-4.01960e+01, -5.12000e-01,  7.93000e-01],\n",
      "       [ 3.62893e+02, -4.97800e+00,  3.42000e-01],\n",
      "       [-3.42795e+02,  5.28200e+00,  2.60000e-01],\n",
      "       [-1.39553e+02, -1.19100e+01, -7.34000e-01],\n",
      "       [ 1.59652e+02,  1.21170e+01, -6.61000e-01]])\n",
      "\n",
      "array([[ -40.196,   -0.512],\n",
      "       [ 362.893,   -4.978],\n",
      "       [-342.795,    5.282],\n",
      "       [-139.553,  -11.91 ],\n",
      "       [ 159.652,   12.117]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined data\n",
    "print('{}\\n'.format(repr(data)))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_obj = PCA() # The value of n_component will be 4. As m is 5 and default is always m-1\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))\n",
    "\n",
    "pca_obj = PCA(n_components=3)\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))\n",
    "\n",
    "pca_obj = PCA(n_components=2)\n",
    "pc = pca_obj.fit_transform(data).round(3)\n",
    "print('{}\\n'.format(repr(pc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big part of data science is classifying observations in a dataset into separate categories, or classes. A popular use case of data classification is in separating a dataset into \"good\" and \"bad\" categories. For example, we can classify a dataset of breast tumors as either malignant or benign.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
      "        1.189e-01],\n",
      "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
      "        8.902e-02],\n",
      "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
      "        8.758e-02],\n",
      "       ...,\n",
      "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
      "        7.820e-02],\n",
      "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
      "        1.240e-01],\n",
      "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
      "        7.039e-02]])\n",
      "\n",
      "Data shape: (569, 30)\n",
      "\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n",
      "\n",
      "Labels shape: (569,)\n",
      "\n",
      "['malignant', 'benign']\n",
      "\n",
      "Malignant shape: (212, 30)\n",
      "\n",
      "Benign shape: (357, 30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc = load_breast_cancer()\n",
    "print('{}\\n'.format(repr(bc.data)))\n",
    "print('Data shape: {}\\n'.format(bc.data.shape))\n",
    "\n",
    "# Class labels\n",
    "print('{}\\n'.format(repr(bc.target)))\n",
    "print('Labels shape: {}\\n'.format(bc.target.shape))\n",
    "\n",
    "# Label names\n",
    "print('{}\\n'.format(list(bc.target_names)))\n",
    "\n",
    "malignant = bc.data[bc.target == 0]\n",
    "print('Malignant shape: {}\\n'.format(malignant.shape))\n",
    "\n",
    "benign = bc.data[bc.target == 1]\n",
    "print('Benign shape: {}\\n'.format(benign.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the bc.data array contains all the dataset values, while the bc.target array contains the class ID labels for each row in bc.data. A class ID of 0 corresponds to a malignant tumor, while a class ID of 1 corresponds to a benign tumor.\n",
    "\n",
    "Using the bc.target class IDs, we separated the dataset into malignant and benign data arrays. In other words, the malignant array contains the rows of bc.data corresponding to the indexes in bc.target containing 0, while the benign array contains the rows of bc.data corresponding to the indexes in bc.target containing 1. There are 212 malignant data observations and 357 benign observations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of linear regression is called least squares regression. This strategy produces a regression model, which is a linear combination of the independent variables, that minimizes the sum of squared residuals between the model's predictions and actual values for the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2100,  800],\n",
      "       [2500,  850],\n",
      "       [1800,  760],\n",
      "       [2000,  800],\n",
      "       [2300,  810]])\n",
      "\n",
      "[10.99, 12.5, 9.99, 10.99, 11.99]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predefined pizza data and prices\n",
    "pizza_prices = [10.99, 12.5, 9.99, 10.99, 11.99]\n",
    "pizza_data = np.array([[2100,  800],\n",
    "                       [2500,  850],\n",
    "                       [1800,  760],\n",
    "                       [2000,  800],\n",
    "                       [2300,  810]])\n",
    "\n",
    "print('{}\\n'.format(repr(pizza_data)))\n",
    "print('{}\\n'.format(repr(pizza_prices)))\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(pizza_data, pizza_prices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling the fit function, the model is ready to use. The predict function allows us to make predictions on new data.\n",
    "\n",
    "We can also get the specific coefficients and intercept for the linear combination using the coef_ and intercept_ properties, respectively.\n",
    "\n",
    "Finally, we can retrieve the coefficient of determination (or R2 value) using the score function applied to the dataset and labels. The R2 value tells us how close of a fit the linear model is to the data, or in other words, how good of a fit the model is for the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([10.86599206, 11.55111111])\n",
      "\n",
      "Coefficients: array([0.00330913, 0.00232937])\n",
      "\n",
      "Intercept: 2.337658730158733\n",
      "\n",
      "R2: 0.9758349388652625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new pizza data\n",
    "new_pizzas = np.array([[2000,  820],\n",
    "                       [2200,  830]])\n",
    "\n",
    "price_predicts = reg.predict(new_pizzas)\n",
    "print('{}\\n'.format(repr(price_predicts)))\n",
    "\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "\n",
    "# Using previously defined pizza_data, pizza_prices\n",
    "r2 = reg.score(pizza_data, pizza_prices)\n",
    "print('R2: {}\\n'.format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional R2 value is a real number between 0 and 1. In scikit-learn it ranges from -∞ to 1, where lower values denote a poorer model fit to the data. The closer the value is to 1, the better the model's fit on the data. In the example above, we see that the model is a near perfect fit for the pizza data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: array([0.00330919, 0.0023288 ])\n",
      "\n",
      "Intercept: 2.3379782896471326\n",
      "\n",
      "R2: 0.9758349388362841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=0.1)\n",
    "reg.fit(pizza_data, pizza_prices)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "r2 = reg.score(pizza_data, pizza_prices)\n",
    "print('R2: {}\\n'.format(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: array([0.00330932, 0.00232767])\n",
      "\n",
      "Intercept: 2.3386168542409536\n",
      "\n",
      "Chosen alpha: 0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "alphas = [0.1, 0.2, 0.3]\n",
    "reg = linear_model.RidgeCV(alphas=alphas)\n",
    "reg.fit(pizza_data, pizza_prices)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "print('Chosen alpha: {}\\n'.format(reg.alpha_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression\n",
    "\n",
    "\n",
    "L1 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: array([0.00345381, 0.00104209])\n",
      "\n",
      "Intercept: 3.062997866538103\n",
      "\n",
      "R2: 0.9756851385558852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined dataset\n",
    "#print('Data shape: {}\\n'.format(pizza_data.shape))\n",
    "#print('Labels shape: {}\\n'.format(pizza_prices.shape))\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit(pizza_data, pizza_prices)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "print('R2: {}\\n'.format(reg.score(pizza_data, pizza_prices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've discussed hyperparameter optimization through cross-validation. Another way to optimize the hyperparameters of a regularized regression model is with Bayesian techniques.\n",
    "\n",
    "In Bayesian statistics, the main idea is to make certain assumptions about the probability distributions of a model's parameters before being fitted on data. These initial distribution assumptions are called priors for the model's parameters.\n",
    "\n",
    "In a Bayesian ridge regression model, there are two hyperparameters to optimize: α and λ. The α hyperparameter serves the same exact purpose as it does for regular ridge regression; namely, it acts as a scaling factor for the penalty term.\n",
    "\n",
    "The λ hyperparameter acts as the precision of the model's weights. Basically, the smaller the λ value, the greater the variance between the individual weight values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: array([0.00347274, 0.00068913])\n",
      "\n",
      "Intercept: 3.3062843444286365\n",
      "\n",
      "R2: 0.9755520913791901\n",
      "\n",
      "Alpha: 41.16588384808269\n",
      "\n",
      "Lambda: 79205.94567539405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined dataset from previous chapter\n",
    "#print('Data shape: {}\\n'.format(data.shape))\n",
    "#print('Labels shape: {}\\n'.format(labels.shape))\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.BayesianRidge()\n",
    "data = pizza_data\n",
    "labels = pizza_prices\n",
    "\n",
    "reg.fit(data, labels)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "print('R2: {}\\n'.format(reg.score(data, labels)))\n",
    "print('Alpha: {}\\n'.format(reg.alpha_))\n",
    "print('Lambda: {}\\n'.format(reg.lambda_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far we've learned about several linear regression models and implemented them with scikit-learn. The logistic regression model, despite its name, is actually a linear model for classification. It is called logistic regression because it performs regression on logits, which then allows us to classify the data based on model probability predictions.\n",
    "\n",
    "\n",
    "For multiclass classification, i.e. when there are more than two labels, we initialize the LogisticRegression object with the multi_class keyword argument. The default value is 'ovr', which signifies a One-Vs-Rest strategy. In multiclass classification, we want to use the 'multinomial' strategy.\n",
    "\n",
    "The code below demonstrates multiclass classification. Note that to use the 'multinomial' strategy, we need to choose a proper solver (see below for details on solvers). In this case, we choose 'lbfgs'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predefined dataset\n",
    "print('Data shape: {}\\n'.format(data.shape))\n",
    "# Multiclass labels\n",
    "print('Labels:\\n{}\\n'.format(repr(labels)))\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LogisticRegression(\n",
    "  solver='lbfgs',\n",
    "  multi_class='multinomial')\n",
    "reg.fit(data, labels)\n",
    "\n",
    "new_data = np.array([\n",
    "  [ 1.8, -0.5, 6.2, 1.4],\n",
    "  [ 3.3,  0.8, 0.1, 2.5]])\n",
    "print('Prediction classes: {}\\n'.format(\n",
    "  repr(reg.predict(new_data))))\n",
    "\n",
    "\n",
    "reg = linear_model.LogisticRegressionCV(\n",
    "  solver='multinomial', max_iter=1000)\n",
    "  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model we've looked at so far is based on creating an optimal linear combination of dataset features for either regression or classification. However, another popular model in data science for both classification and regression is the decision tree. It is a binary tree where each node of the tree decides on a particular feature of the dataset, and we descend to the node's left or right child depending on the feature's value.\n",
    "\n",
    "If a feature is boolean, we go left or right from the node depending on if the feature value is true or false. If a feature is numeric, we can decide to go left or right based on a decision boundary for the feature (e.g. go left if the feature value is less than 1, otherwise go right).\n",
    "\n",
    "The leaves of the decision tree determine the class label to predict (in classification) or the real number value to predict (in regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn import tree\n",
    "clf_tree1 = tree.DecisionTreeClassifier()\n",
    "reg_tree1 = tree.DecisionTreeRegressor()\n",
    "clf_tree2 = tree.DecisionTreeClassifier(\n",
    "  max_depth=8)  # max depth of 8\n",
    "reg_tree2 = tree.DecisionTreeRegressor(\n",
    "  max_depth=5)  # max depth of 5\n",
    "\n",
    "# predefined dataset\n",
    "print('Data shape: {}\\n'.format(data.shape))\n",
    "# Binary labels\n",
    "print('Labels:\\n{}\\n'.format(repr(labels)))\n",
    "clf_tree1.fit(data, labels)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_depth keyword argument lets us manually set the maximum number of layers allowed in the decision tree (i.e. the tree's maximum depth). The default value is None, meaning that the decision tree will continue to be constructed until no nodes can have anymore children. Since large decision trees are prone to overfit data, it can be beneficial to manually set a maximum depth for the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a decision tree makes decisions based on feature values, the question now becomes how we choose the features to decide on at each node. In general terms, we want to choose the feature value that \"best\" splits the remaining dataset at each node.\n",
    "\n",
    "How we define \"best\" depends on the decision tree algorithm that's used. Since scikit-learn uses the CART algorithm, we use Gini Impurity, MSE (mean squared error), and MAE (mean absolute error) to decide on the best feature at each node.\n",
    "\n",
    "Specifically, for classification trees we choose the feature at each node that minimizes the remaining dataset observations' Gini Impurity. For regression trees we choose the feature at each node that minimizes the remaining dataset observations' MSE or MAE, depending on which you choose to use (the default for DecisionTreeRegressor is MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 9.3 ,  0.8 ],\n",
      "       [10.2 ,  0.5 ],\n",
      "       [ 9.5 ,  0.77],\n",
      "       [ 7.7 ,  0.9 ],\n",
      "       [ 8.7 ,  0.9 ],\n",
      "       [10.1 ,  0.4 ]])\n",
      "\n",
      "array([1.6, 1.4, 1.6, 1.1, 1.2, 1.5])\n",
      "\n",
      "array([[8.3 , 0.8 ],\n",
      "       [9.1 , 0.68]])\n",
      "\n",
      "array([1.2, 1.3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [10.2 ,  0.5 ],\n",
    "  [ 8.7 ,  0.9 ],\n",
    "  [ 9.3 ,  0.8 ],\n",
    "  [10.1 ,  0.4 ],\n",
    "  [ 9.5 ,  0.77],\n",
    "  [ 9.1 ,  0.68],\n",
    "  [ 7.7 ,  0.9 ],\n",
    "  [ 8.3 ,  0.8 ]])\n",
    "labels = np.array(\n",
    "  [1.4, 1.2, 1.6, 1.5, 1.6, 1.3, 1.1, 1.2])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "split_dataset = train_test_split(data, labels)\n",
    "train_data = split_dataset[0]\n",
    "test_data = split_dataset[1]\n",
    "train_labels = split_dataset[2]\n",
    "test_labels = split_dataset[3]\n",
    "\n",
    "print('{}\\n'.format(repr(train_data)))\n",
    "print('{}\\n'.format(repr(train_labels)))\n",
    "print('{}\\n'.format(repr(test_data)))\n",
    "print('{}\\n'.format(repr(test_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the train_test_split function randomly shuffles the dataset and corresponding labels prior to splitting. This is good practice to remove any systematic orderings in the dataset, which could potentially impact the model into training on the orderings rather than the actual data.\n",
    "\n",
    "The default size of the testing set is 25% of the original dataset. We can use the test_size keyword argument to manually specify the proportion of the original dataset that will go into the testing set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it's not enough to just have a single testing set for model evaluation. Having additional sets of data for evaluation gives us a more accurate measurement of how good the model is for the original dataset.\n",
    "\n",
    "If the original dataset is big enough, we can actually split it into three subsets: training, testing, and validation. The validation set is about the same size as the testing set, and it is used for evaluating the model after training. The testing set is then used for final evaluation once the model is done training and tuning.\n",
    "\n",
    "However, partitioning the original dataset into three distinct sets will cut into the size of the training set. This can reduce the performance of the model if our original dataset is not large enough. A solution to this problem is cross-validation (CV).\n",
    "\n",
    "Cross-validation creates synthetic validation sets by partitioning the training set into multiple smaller subsets. One of the most common algorithms for cross-validation, K-Fold CV, partitions the training set into k approximately equal sized subsets (referred to as folds). There are k \"rounds\" of the algorithm, and each \"round\" chooses one of the k subsets for the validation set (a different subset is chosen each round), while the remaining k - 1 subsets are aggregated into the round's training set and used to train the model.\n",
    "\n",
    "\n",
    "Each round of the K-Fold algorithm, the model is trained on that round's training set (the combined training folds) and then evaluated on the single validation fold. The evaluation metric depends on the model. For classification models, this is usually classification accuracy on the validation set. For regression models, this can either be the model's mean squared error, mean absolute error, or R2 value on the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn import linear_model\\nfrom sklearn.model_selection import cross_val_score\\nclf = linear_model.LogisticRegression()\\n# Predefined data and labels\\ncv_score = cross_val_score(clf, data, labels, cv=3)  # k = 3\\n\\nprint('{}\\n'.format(repr(cv_score)))\\n\\n\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = linear_model.LogisticRegression()\n",
    "# Predefined data and labels\n",
    "cv_score = cross_val_score(clf, data, labels, cv=3)  # k = 3\n",
    "\n",
    "print('{}\\n'.format(repr(cv_score)))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't call fit with the model prior to using cross_val_score. This is because the cross_val_score function will use fit for training the model each round.\n",
    "\n",
    "For classification models, the cross_val_score function will apply a special form of the K-Fold algorithm called stratified K-Fold. This just means that each fold will contain approximately the same class distribution as the original dataset. For example, if the original dataset contained 60% class 0 data observations and 40% class 1, each fold of the stratified K-Fold algorithm will have about the same 60-40 split between class 0 and class 1 data observations.\n",
    "\n",
    "While cross-validation gives us a better measurement of the model's fit on the original dataset, it can be very time-consuming when used on large datasets. For large enough datasets, it is better to just split it into training, validation, and testing sets, and then use the validation set for evaluating the model before it is finalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying CV to Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nis_clf = True  # for classification\\nfor depth in range(3, 8):\\n  # Predefined data and labels\\n  scores = cv_decision_tree(\\n    is_clf, data, labels, depth, 5)  # k = 5\\n  mean = scores.mean()  # Mean acc across folds\\n  std_2 = 2 * scores.std()  # 2 std devs\\n  print('95% C.I. for depth {}: {} +/- {:.2f}\\n'.format(\\n    depth, mean, std_2))\\n    \\n\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "is_clf = True  # for classification\n",
    "for depth in range(3, 8):\n",
    "  # Predefined data and labels\n",
    "  scores = cv_decision_tree(\n",
    "    is_clf, data, labels, depth, 5)  # k = 5\n",
    "  mean = scores.mean()  # Mean acc across folds\n",
    "  std_2 = 2 * scores.std()  # 2 std devs\n",
    "  print('95% C.I. for depth {}: {} +/- {:.2f}\\n'.format(\n",
    "    depth, mean, std_2))\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_decision_tree(is_clf, data, labels,\n",
    "                     max_depth, cv):\n",
    "    if is_clf:\n",
    "        d_tree = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "    else:\n",
    "        d_tree = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    scores = cross_val_score(d_tree, data, labels, cv=cv)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification models, we use the classification accuracy on the test set as the evaluation metric. For regression models, we normally use either the R2 value, mean squared error, or mean absolute error on the test set. The most commonly used regression metric is mean absolute error, since it represents the natural definition of error. We use mean squared error when we want to penalize really bad predictions, since the error is squared. We use the R2 value when we want to evaluate the fit of the regression model on the data.\n",
    "\n",
    "The metrics module of scikit-learn provides functions for each of these metrics. Each of the evaluation functions takes in the actual testing labels as the first argument and the predictions as the second argument.\n",
    "\n",
    "The code below evaluates a regression model's predictions based on the testing labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nreg = tree.DecisionTreeRegressor()\\n# predefined train and test sets\\nreg.fit(train_data, train_labels)\\npredictions = reg.predict(test_data)\\n\\nfrom sklearn import metrics\\nr2 = metrics.r2_score(test_labels, predictions)\\nprint('R2: {}\\n'.format(r2))\\nmse = metrics.mean_squared_error(\\n  test_labels, predictions)\\nprint('MSE: {}\\n'.format(mse))\\nmae = metrics.mean_absolute_error(\\n  test_labels, predictions)\\nprint('MAE: {}\\n'.format(mae))\\n\\nclf = tree.DecisionTreeClassifier()\\n# predefined train and test sets\\nclf.fit(train_data, train_labels)\\npredictions = clf.predict(test_data)\\n\\nfrom sklearn import metrics\\nacc = metrics.accuracy_score(test_labels, predictions)\\nprint('Accuracy: {}\\n'.format(acc))\\n\\n\""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reg = tree.DecisionTreeRegressor()\n",
    "# predefined train and test sets\n",
    "reg.fit(train_data, train_labels)\n",
    "predictions = reg.predict(test_data)\n",
    "\n",
    "from sklearn import metrics\n",
    "r2 = metrics.r2_score(test_labels, predictions)\n",
    "print('R2: {}\\n'.format(r2))\n",
    "mse = metrics.mean_squared_error(\n",
    "  test_labels, predictions)\n",
    "print('MSE: {}\\n'.format(mse))\n",
    "mae = metrics.mean_absolute_error(\n",
    "  test_labels, predictions)\n",
    "print('MAE: {}\\n'.format(mae))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# predefined train and test sets\n",
    "clf.fit(train_data, train_labels)\n",
    "predictions = clf.predict(test_data)\n",
    "\n",
    "from sklearn import metrics\n",
    "acc = metrics.accuracy_score(test_labels, predictions)\n",
    "print('Accuracy: {}\\n'.format(acc))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our application requires us to absolutely obtain the best hyperparameters of a model, and if the dataset is small enough, we can apply an exhaustive grid search for tuning hyperparameters. For the grid search cross-validation, we specify possible values for each hyperparameter, and then the search will go through each possible combination of the hyperparameters and return the model with the best combination.\n",
    "\n",
    "We implement grid search cross-validation with the GridSearchCV object (part of the model_selection module).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nreg = linear_model.BayesianRidge()\\nparams = {\\n  'alpha_1':[0.1,0.2,0.3],\\n  'alpha_2':[0.1,0.2,0.3]\\n}\\nreg_cv = GridSearchCV(reg, params, cv=5, iid=False)\\n# predefined train and test sets\\nreg_cv.fit(train_data, train_labels)\\nprint(reg_cv.best_params_)\\n\\n\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reg = linear_model.BayesianRidge()\n",
    "params = {\n",
    "  'alpha_1':[0.1,0.2,0.3],\n",
    "  'alpha_2':[0.1,0.2,0.3]\n",
    "}\n",
    "reg_cv = GridSearchCV(reg, params, cv=5, iid=False)\n",
    "# predefined train and test sets\n",
    "reg_cv.fit(train_data, train_labels)\n",
    "print(reg_cv.best_params_)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code example above, we searched through each possible pair of α1 and α2 values based on the two lists in the params dictionary. The search resulted in an α1 value of 0.3 and an α2 value of 0.1. For each of the models we've covered, you can take a look at their respective scikit-learn code documentation pages to determine the model's hyperparameters that can be used as the params argument for GridSearchCV.\n",
    "\n",
    "The cv keyword argument represents the number of folds used in the K-Fold cross-validation for grid search. The iid keyword argument relates to how the cross-validation score is calculated. We use False to match the standard definition of cross-validation. Note that in later updates of scikit-learn, the iid argument will be removed from GridSearchCV.\n",
    "\n",
    "Since exhaustive grid search performs cross-validation on each possible hyperparameter value combination, it can be incredibly slow for larger datasets. It should only be used if the dataset is reasonably small and it is important to choose the best hyperparameter combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Clustering section, you will be using unsupervised learning methods, i.e. methods of extracting insights from unlabeled datasets. Specifically, you will learn about different clustering algorithms and how they are able to group together similar data observations.\n",
    "\n",
    "A. Unsupervised learning\n",
    "So far, we've only used supervised learning methods, since we've exclusively been dealing with labeled datasets. However, in the real world many datasets are completely unlabeled, since labeling datasets involves additional work and foresight. Rather than just ignoring all these unlabeled datasets, we can still extract meaningful insights using unsupervised learning.\n",
    "\n",
    "Since we only have data observations to work with, and no labels, unsupervised learning methods are centered around finding similarities/differences between data observations and making inferences based on those findings. The most commonly used form of unsupervised learning is clustering. As the name suggests, clustering algorithms will cluster the data into distinct groups (clusters), where each cluster consists of similar data observations.\n",
    "\n",
    "Clustering is used in many different applications, from anomaly detection (i.e. detecting real vs. fraudulent data) to market research (e.g. grouping customers together based on their purchase history). In the upcoming chapters, you'll learn about a variety of commonly used clustering algorithms in data science, as well as other tools for finding similarities between data observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity for two vectors, u and v, is calculated as the dot product between the L2-normalization of the vectors. The exact formula for cosine similarity is:\n",
    "\n",
    "$cos(u, v) = \\frac{u \\cdot v}{\\lVert u \\rVert_2 \\lVert v \\rVert_2}$\n",
    "\n",
    "\n",
    "where ||u||2 represents the L2 norm of u and ||v||2 represents the L2 norm of v.\n",
    "\n",
    "\n",
    "In scikit-learn, cosine similarity is implemented via the cosine_similarity function (which is part of the metrics.pairwise module). It calculates the cosine similarities for pairs of data observations in a single dataset, or pairs of data observations between two datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.        ,  0.99992743, -0.99659724, -0.26311741],\n",
      "       [ 0.99992743,  1.        , -0.99751792, -0.27472113],\n",
      "       [-0.99659724, -0.99751792,  1.        ,  0.34174306],\n",
      "       [-0.26311741, -0.27472113,  0.34174306,  1.        ]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = np.array([\n",
    "  [ 1.1,  0.3],\n",
    "  [ 2.1,  0.6],\n",
    "  [-1.1, -0.4],\n",
    "  [ 0. , -3.2]])\n",
    "cos_sims = cosine_similarity(data)\n",
    "print('{}\\n'.format(repr(cos_sims)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we only pass in one dataset into cosine_similarity, the function will compute cosine similarities between pairs of observations within the dataset. In the code above, we passed in data (which contains 4 data observations), so the output of cosine_similarity is a 4x4 array of cosine similarity values.\n",
    "\n",
    "The value at index (i, j) of cos_sims is the cosine similarity between data observations i and j in data. Since cosine similarity is symmetric, the cos_sims array contains the same values at index (i, j) and (j, i).\n",
    "\n",
    "Note that the cosine similarity between a data observation and itself is 1, unless the data observation contains only 0's as feature values (in which case the cosine similarity is 0).\n",
    "\n",
    "If we decide to pass in two datasets (with equal numbers of columns) into cosine_similarity, the function will compute the cosine similarities for pairs of data observations between the two datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.9993819 ,  0.99973508, -0.91578821],\n",
      "       [ 0.99888586,  0.99993982, -0.9108828 ],\n",
      "       [-0.99308366, -0.9982304 ,  0.87956492],\n",
      "       [-0.22903933, -0.28525359, -0.14654866]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = np.array([\n",
    "  [ 1.1,  0.3],\n",
    "  [ 2.1,  0.6],\n",
    "  [-1.1, -0.4],\n",
    "  [ 0. , -3.2]])\n",
    "data2 = np.array([\n",
    "  [ 1.7,  0.4],\n",
    "  [ 4.2, 1.25],\n",
    "  [-8.1,  1.2]])\n",
    "cos_sims = cosine_similarity(data, data2)\n",
    "print('{}\\n'.format(repr(cos_sims)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the value at index (i, j) of cos_sims is the cosine similarity between data observation i in data and data observation j in data2. Note that cos_sims is a 4x3 array, since data contains 4 data observations and data2 contains 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncos_sims = cosine_similarity(data)\\nnp.fill_diagonal(cos_sims, 0)\\nsimilar_indexes = cos_sims.argmax(axis=1)\\n\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cos_sims = cosine_similarity(data)\n",
    "np.fill_diagonal(cos_sims, 0)\n",
    "similar_indexes = cos_sims.argmax(axis=1)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal in cos_sims represents the similarities between each observation and itself. We'll substitute each diagonal value with 0, so that for each data observation we find the most similar observation besides itself.\n",
    "\n",
    "In NumPy, the function fill_diagonal allows us to fill the diagonal of an array with a specified value.\n",
    "\n",
    "Call np.fill_diagonal with cos_sims as the first argument and 0 as the second argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 1, we mentioned that clustering is a method for grouping together similar data observations. Another method for finding similar data observations is the nearest neighbors approach. With this approach, we find the k most similar data observations (i.e. neighbors) for a given data observation (where k represents the number of neighbors).\n",
    "\n",
    "In scikit-learn, we implement the nearest neighbors approach with the NearestNeighbors object (part of the neighbors module).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[7, 4, 0, 6, 9]])\n",
      "\n",
      "array([[0.17320508, 0.24494897, 0.24494897, 0.45825757, 0.46904158]])\n",
      "\n",
      "array([[7, 4, 0, 6, 9]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [5.1, 3.5, 1.4, 0.2],\n",
    "  [4.9, 3. , 1.4, 0.2],\n",
    "  [4.7, 3.2, 1.3, 0.2],\n",
    "  [4.6, 3.1, 1.5, 0.2],\n",
    "  [5. , 3.6, 1.4, 0.2],\n",
    "  [5.4, 3.9, 1.7, 0.4],\n",
    "  [4.6, 3.4, 1.4, 0.3],\n",
    "  [5. , 3.4, 1.5, 0.2],\n",
    "  [4.4, 2.9, 1.4, 0.2],\n",
    "  [4.9, 3.1, 1.5, 0.1]])\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors()\n",
    "nbrs.fit(data)\n",
    "new_obs = np.array([[5. , 3.5, 1.6, 0.3]])\n",
    "dists, knbrs = nbrs.kneighbors(new_obs)\n",
    "\n",
    "# nearest neighbors indexes\n",
    "print('{}\\n'.format(repr(knbrs)))\n",
    "# nearest neighbor distances\n",
    "print('{}\\n'.format(repr(dists)))\n",
    "\n",
    "only_nbrs = nbrs.kneighbors(new_obs,\n",
    "                            return_distance=False)\n",
    "print('{}\\n'.format(repr(only_nbrs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NearestNeighbors object is fitted with a dataset, which is then used as the pool of possible neighbors for new data observations. The kneighbors function takes in new data observation(s) and returns the k nearest neighbors along with their respective distances from the input data observations. Note that the nearest neighbors are the neighbors with the smallest distances from the input data observation. We can choose not to return the distances by setting the return_distance keyword argument to False.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[7, 0],\n",
      "       [9, 2]])\n",
      "\n",
      "array([[0.17320508, 0.24494897],\n",
      "       [0.14142136, 0.24494897]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [5.1, 3.5, 1.4, 0.2],\n",
    "  [4.9, 3. , 1.4, 0.2],\n",
    "  [4.7, 3.2, 1.3, 0.2],\n",
    "  [4.6, 3.1, 1.5, 0.2],\n",
    "  [5. , 3.6, 1.4, 0.2],\n",
    "  [5.4, 3.9, 1.7, 0.4],\n",
    "  [4.6, 3.4, 1.4, 0.3],\n",
    "  [5. , 3.4, 1.5, 0.2],\n",
    "  [4.4, 2.9, 1.4, 0.2],\n",
    "  [4.9, 3.1, 1.5, 0.1]])\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=2)\n",
    "nbrs.fit(data)\n",
    "new_obs = np.array([\n",
    "  [5. , 3.5, 1.6, 0.3],\n",
    "  [4.8, 3.2, 1.5, 0.1]])\n",
    "dists, knbrs = nbrs.kneighbors(new_obs)\n",
    "\n",
    "# nearest neighbors indexes\n",
    "print('{}\\n'.format(repr(knbrs)))\n",
    "# nearest neighbor distances\n",
    "print('{}\\n'.format(repr(dists)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind clustering data is pretty simple: partition a dataset into groups of similar data observations. How we go about finding these clusters is a bit more complex, since there are a number of different methods for clustering datasets.\n",
    "\n",
    "The most well-known clustering method is K-means clustering. The K-means clustering algorithm will separate the data into K clusters (the number of clusters is chosen by the user) using cluster means, also known as centroids.\n",
    "\n",
    "These centroids represent the \"centers\" of each cluster. Specifically, a cluster's centroid is equal to the average of all the data observations within the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster:\n",
      "array([[ 1.2,  0.6],\n",
      "       [ 2.4,  0.8],\n",
      "       [-1.6,  1.4],\n",
      "       [ 0. ,  1.2]])\n",
      "\n",
      "Centroid:\n",
      "array([0.5, 1. ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = np.array([\n",
    "  [ 1.2, 0.6],\n",
    "  [ 2.4, 0.8],\n",
    "  [-1.6, 1.4],\n",
    "  [ 0. , 1.2]])\n",
    "print('Cluster:\\n{}\\n'.format(repr(cluster)))\n",
    "\n",
    "centroid = cluster.mean(axis=0)\n",
    "print('Centroid:\\n{}\\n'.format(repr(centroid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering algorithm is an iterative process. Each iteration, the algorithm will assign each data observation to the cluster with the closest centroid to the observation (using the regular distance metric).\n",
    "\n",
    "Then it updates each centroid to be equal to the new average of the data observations in the cluster. Note that at the beginning of the algorithm, the cluster centroids are either randomly initialized or (better) initialized using the K-means++ algorithm. The clustering process stops when there are no more changes in cluster assignment for any data observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 1, 1, 1, 0, 2, 1, 0, 1, 1], dtype=int32)\n",
      "\n",
      "array([[5.03333333, 3.5       , 1.43333333, 0.2       ],\n",
      "       [4.68333333, 3.11666667, 1.41666667, 0.2       ],\n",
      "       [5.4       , 3.9       , 1.7       , 0.4       ]])\n",
      "\n",
      "array([2, 2], dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# predefined data\n",
    "kmeans.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(kmeans.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(kmeans.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(kmeans.predict(new_obs))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Mini-batch clustering\n",
    "\n",
    "When working with very large datasets, regular K-means clustering can be quite slow. To reduce the computation time, we can perform mini-batch K-means clustering, which is just regular K-means clustering applied to randomly sampled subsets of the data (mini-batches) at a time.\n",
    "\n",
    "There is a trade-off in using mini-batch clustering, as the results may not be as good as regular K-means clustering. However, in practice the difference in quality is negligible, so mini-batch clustering is usually the choice when dealing with large datasets.\n",
    "\n",
    "In scikit-learn, mini-batch K-means clustering is implemented using the MiniBatchKMeans object (also part of the cluster module). It is used in the same way as the regular KMeans object, with an additional batch_size keyword argument during initialization that allows us to specify the size of each mini-batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 0, 0, 0, 2, 1, 0, 2, 0, 0], dtype=int32)\n",
      "\n",
      "array([[4.6755102 , 3.13061224, 1.42040816, 0.2       ],\n",
      "       [5.4       , 3.9       , 1.7       , 0.4       ],\n",
      "       [5.02142857, 3.50357143, 1.42678571, 0.20357143]])\n",
      "\n",
      "array([1, 1], dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=3, batch_size=10)\n",
    "# predefined data\n",
    "kmeans.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(kmeans.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(kmeans.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(kmeans.predict(new_obs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(data, n_clusters, batch_size):\n",
    "    if batch_size is None:\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "    else:\n",
    "        kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n",
    "                             batch_size=batch_size)\n",
    "        kmeans.fit(data)\n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. K-means vs. hierarchical clustering\n",
    "\n",
    "A major assumption that the K-means clustering algorithm makes is that the dataset consists of spherical (i.e. circular) clusters. With this assumption, the K-means algorithm will create clusters of data observations that are circular around the centroids. However, real life data often does not contain spherical clusters, meaning that K-means clustering might end up producing inaccurate clusters due to its assumption.\n",
    "\n",
    "An alternative to K-means clustering is hierarchical clustering. Hierarchical clustering allows us to cluster any type of data, since it doesn't make any assumptions about the data or clusters.\n",
    "\n",
    "There are two approaches to hierarchical clustering: bottom-up (divisive) and top-down (agglomerative). The divisive approach initially treats all the data as a single cluster, then repeatedly splits it into smaller clusters until we reach the desired number of clusters. The agglomerative approach initially treats each data observation as its own cluster, then repeatedly merges the two most similar clusters until we reach the desired number of clusters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, agglomerative clustering is implemented through the AgglomerativeClustering object (part of the cluster module). Similar to the KMeans object from the previous chapter, we specify the number of clusters with the n_clusters keyword argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 0, 0, 0, 1, 2, 0, 1, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "# predefined data\n",
    "agg.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(agg.labels_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since agglomerative clustering doesn't make use of centroids, there's no cluster_centers_ attribute in the AgglomerativeClustering object. There's also no predict function for making cluster predictions on new data (since K-means clustering makes use of its final centroids for new data predictions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift Clustering\n",
    "\n",
    "Use mean shift clustering to determine the optimal number of clusters.\n",
    "\n",
    "A. Choosing the number of clusters\n",
    "\n",
    "Each of the clustering algorithms we've used so far require us to pass in the number of clusters. This is fine if we already know the number of clusters we want, or have a good guess for the number of clusters. However, if we don't have a good idea of what the actual number of clusters for the dataset should be, there exist algorithms that can automatically choose the number of clusters for us.\n",
    "\n",
    "One such algorithm is the mean shift clustering algorithm. Like the K-means clustering algorithm, the mean shift algorithm is based on finding cluster centroids. Since we don't provide the number of clusters, the algorithm will look for \"blobs\" in the data that can be potential candidates for clusters.\n",
    "\n",
    "Using these \"blobs\", the algorithm finds a number of candidate centroids. It then removes the candidates that are basically duplicates of others to form the final set of centroids. The final set of centroids determines the number of clusters as well as the dataset cluster assignments (data observations are assigned to the nearest centroid).\n",
    "\n",
    "In scikit-learn, the mean shift algorithm is implemented with the MeanShift object (part of the cluster module). Since the algorithm doesn't require us to pass in the number of clusters, we can initialize the MeanShift with no arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 0, 0, 0, 1, 2, 0, 1, 0, 0])\n",
      "\n",
      "array([[4.74      , 3.16      , 1.42      , 0.2       ],\n",
      "       [5.03333333, 3.5       , 1.43333333, 0.2       ],\n",
      "       [5.4       , 3.9       , 1.7       , 0.4       ]])\n",
      "\n",
      "array([2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "mean_shift = MeanShift()\n",
    "# predefined data\n",
    "mean_shift.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(mean_shift.labels_)))\n",
    "\n",
    "# centroids\n",
    "print('{}\\n'.format(repr(mean_shift.cluster_centers_)))\n",
    "\n",
    "new_obs = np.array([\n",
    "  [5.1, 3.2, 1.7, 1.9],\n",
    "  [6.9, 3.2, 5.3, 2.2]])\n",
    "# predict clusters\n",
    "print('{}\\n'.format(repr(mean_shift.predict(new_obs))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Clustering by density\n",
    "\n",
    "The mean shift clustering algorithm in the previous chapter usually performs sufficiently well and can choose a reasonable number of clusters. However, it is not very scalable due to computation time and still makes the assumption that clusters have a \"blob\"-like shape (although this assumption is not as strong as the one made by K-means).\n",
    "\n",
    "Another clustering algorithm that also automatically chooses the number of clusters is DBSCAN. DBSCAN clusters data by finding dense regions in the dataset. Regions in the dataset with many closely packed data observations are considered high-density regions, while regions with sparse data are considered low-density regions.\n",
    "\n",
    "The DBSCAN algorithm treats high-density regions as clusters in the dataset, and low-density regions as the area between clusters (so observations in the low-density regions are treated as noise and not placed in a cluster).\n",
    "\n",
    "High-density regions are defined by core samples, which are just data observations with many neighbors. Each cluster consists of several core samples and all the observations that are neighbors to a core sample.\n",
    "\n",
    "Unlike the mean shift algorithm, the DBSCAN algorithm is both highly scalable and makes no assumptions about the underlying shape of clusters in the dataset.\n",
    "\n",
    "B. Neighbors and core samples\n",
    "\n",
    "The exact definition of \"neighbor\" and \"core sample\" depends on what we want in our clusters. We specify the maximum distance, ε, between two data observations that are considered neighbors. Smaller distances result in smaller and more tightly packed clusters. We also specify the minimum number of points in the neighborhood of a data observation for the observation to be considered a core sample (the neighborhood consists of the data observation and all its neighbors).\n",
    "\n",
    "In scikit-learn, we implement DBSCAN with the DBSCAN object (part of the cluster module). The object is initialized with the keyword arguments eps (representing the value of ε) and min_samples (representing the minimum size of a core sample's neighborhood).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfrom sklearn.cluster import DBSCAN\\ndbscan = DBSCAN(eps=1.2, min_samples=30)\\n# predefined data\\ndbscan.fit(data)\\n\\n# cluster assignments\\nprint('{}\\n'.format(repr(dbscan.labels_)))\\n\\n# core samples\\nprint('{}\\n'.format(repr(dbscan.core_sample_indices_)))\\nnum_core_samples = len(dbscan.core_sample_indices_)\\nprint('Num core samples: {}\\n'.format(num_core_samples))\\n\\n\\n\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=30)\n",
    "# predefined data\n",
    "dbscan.fit(data)\n",
    "\n",
    "# cluster assignments\n",
    "print('{}\\n'.format(repr(dbscan.labels_)))\n",
    "\n",
    "# core samples\n",
    "print('{}\\n'.format(repr(dbscan.core_sample_indices_)))\n",
    "num_core_samples = len(dbscan.core_sample_indices_)\n",
    "print('Num core samples: {}\\n'.format(num_core_samples))\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Evaluation metrics\n",
    "\n",
    "When we don't have access to any true cluster assignments (labels), the best we can do to evaluate clusters is to just take a look at them and see if they make sense with respect to the dataset and domain. However, if we do have access to the true cluster labels for the data observations, we can apply a number of metrics to evaluate our clustering algorithm.\n",
    "\n",
    "One popular evaluation metric is the adjusted Rand index. The regular Rand index gives a measurement of similarity between the true clustering assignments (true labels) and the predicted clustering assignments (predicted labels). The adjusted Rand index (ARI) is a corrected-for-chance version of the regular one, meaning that the score is adjusted so that random clustering assignments will not have a good score.\n",
    "\n",
    "The ARI value ranges from -1 to 1, inclusive. Negative scores represent bad labelings, random labelings will get a score near 0, and perfect labelings get a score of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424246\n",
      "\n",
      "0.24242424242424246\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "-0.12903225806451613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# symmetric\n",
    "ari = adjusted_rand_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ari = adjusted_rand_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ari = adjusted_rand_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ari = adjusted_rand_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])\n",
    "# Bad labeling\n",
    "pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])\n",
    "ari = adjusted_rand_score(true_labels2, pred_labels2)\n",
    "print('{}\\n'.format(ari))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the adjusted_rand_score function is symmetric. This means that changing the order of the arguments will not affect the score. Furthermore, permutations in the labeling or changing the label names (i.e. 0 and 1 vs. 1 and 3) does not affect the score.\n",
    "\n",
    "Another common clustering evaluation metric is adjusted mutual information (AMI). It is implemented in scikit-learn with the adjusted_mutual_info_score function (also part of the cluster module). Like adjusted_rand_score, the function is symmetric and oblivious to permutations and renamed labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29879245817089006\n",
      "\n",
      "0.29879245817089006\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "-0.166666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ami = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# symmetric\n",
    "ami = adjusted_mutual_info_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ami = adjusted_mutual_info_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ami = adjusted_mutual_info_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ami = adjusted_mutual_info_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])\n",
    "# Bad labeling\n",
    "pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])\n",
    "ami = adjusted_mutual_info_score(true_labels2, pred_labels2)\n",
    "print('{}\\n'.format(ami))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ARI and AMI metrics are very similar. They both assign a score of 1.0 to perfect labelings, a score near 0.0 to random labelings, and negative scores to poor labelings.\n",
    "\n",
    "A general rule of thumb of when to use which: ARI is used when the true clusters are large and approximately equal sized, while AMI is used when the true clusters are unbalanced in size and there exist small clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Agglomerative feature clustering\n",
    "\n",
    "In the Data Preprocessing section, we used PCA to perform feature dimensionality reduction on datasets. We can also perform feature dimensionality reduction using agglomerative clustering. By merging common features into clusters, we reduce the number of total features while still maintaining most of the original information from the dataset.\n",
    "\n",
    "In scikit-learn, we perform agglomerative clustering on features using the FeatureAgglomeration object (part of the cluster module). When initializing the object, n_clusters keyword argument (which represents the number of final clusters) is used to specify the new feature dimension of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
